{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#documentacao-dos-projetos-de-machine-learning","title":"Documenta\u00e7\u00e3o dos Projetos de Machine Learning","text":""},{"location":"#autor","title":"Autor","text":"<p>Luiz Felipe Pimenta Berrettini</p> <p>Estudante do 4\u00b0 semestre de Ci\u00eancias de Dados e Neg\u00f3cios (CDN) na ESPM (Escola Superior de Propaganda e Marketing). </p> <p>Projetos de machine learning realizados em 2025.2, orientados e supervisionados pelo professor Humberto Sandmann.</p>"},{"location":"#entregas","title":"Entregas","text":"<ul> <li> Decision Tree - Data 29/08/2025</li> <li> KNN - Data 16/09/2025</li> <li> K-means e M\u00e9tricas de Avalia\u00e7\u00e3o - Data 28/09/2025</li> </ul>"},{"location":"#referencias","title":"Refer\u00eancias","text":"<p>Material for MkDocs</p>"},{"location":"decision-tree/main/","title":"Projeto 1 - Decision Tree","text":""},{"location":"decision-tree/main/#modelo-de-machine-learning-arvore-de-decisoes","title":"Modelo de Machine Learning - \u00c1rvore de Decis\u00f5es","text":"<p>Para esse projeto, foi utilizado um dataset obtido no Kaggle. Os dados usados podem ser baixados aqui.</p>"},{"location":"decision-tree/main/#objetivo","title":"Objetivo","text":"<p>O dataset apresenta diversos dados relacionados \u00e0 cada um dos personagens da s\u00e9rie de livros A Song of Ice and Fire, escrita por George R. R. Martin, inspira\u00e7\u00e3o para a famosa s\u00e9rie Game of Thrones. O objetivo dessa an\u00e1lise \u00e9 o modelo fazer a predi\u00e7\u00e3o da import\u00e2ncia do personagem para a s\u00e9rie no sentido de trama. Uma vari\u00e1vel categ\u00f3rica ser\u00e1 criada a partir das vari\u00e1veis presentes no dataset, classificando a relev\u00e2ncia do personagem. Essa vari\u00e1vel ser\u00e1 avaliada pelo modelo de Machine Learning.</p>"},{"location":"decision-tree/main/#workflow","title":"Workflow","text":"<p>Os pontos \"etapas\" s\u00e3o o passo-a-passo da realiza\u00e7\u00e3o do projeto.</p>"},{"location":"decision-tree/main/#etapa-1-exploracao-de-dados","title":"Etapa 1 - Explora\u00e7\u00e3o de Dados","text":"<p>O dataset escolhido \u00e9 composto por 1946 linhas e 30 colunas, contendo um personagem distinto em cada linha e diversas informa\u00e7\u00f5es sobre cada um.</p>"},{"location":"decision-tree/main/#colunas-do-dataset","title":"Colunas do dataset","text":"Coluna Tipo Descri\u00e7\u00e3o <code>S.No</code> Inteiro Identificador \u00fanico do personagem <code>plod</code> Float Valor n\u00e3o especificado <code>name</code> String Nome do personagem <code>title</code> String Alcunha atribu\u00edda ao personagem dentro do mundo <code>gender</code> Bin\u00e1rio Sexo do personagem: 0 = feminino, 1 = masculino <code>culture</code> String Grupo social ao qual o personagem pertence <code>dateOfBirth</code> Inteiro Data de nascimento. Valores positivos = depois do ano 0, negativos = antes do ano 0 <code>DateoFdeath</code> Inteiro Data de morte. Valores positivos = depois do ano 0, negativos = antes do ano 0 <code>mother</code> String Nome da m\u00e3e do personagem <code>father</code> String Nome do pai do personagem <code>heir</code> String Nome do herdeiro do personagem <code>house</code> String Nome da casa \u00e0 qual o personagem pertence <code>spouse</code> String Nome do c\u00f4njuge do personagem <code>book1</code> Bin\u00e1rio Indica se o personagem apareceu no primeiro livro <code>book2</code> Bin\u00e1rio Indica se o personagem apareceu no segundo livro <code>book3</code> Bin\u00e1rio Indica se o personagem apareceu no terceiro livro <code>book4</code> Bin\u00e1rio Indica se o personagem apareceu no quarto livro <code>book5</code> Bin\u00e1rio Indica se o personagem apareceu no quinto livro <code>isAliveMother</code> Bin\u00e1rio Indica se a m\u00e3e do personagem est\u00e1 viva <code>isAliveFather</code> Bin\u00e1rio Indica se o pai do personagem est\u00e1 vivo <code>isAliveHeir</code> Bin\u00e1rio Indica se o herdeiro do personagem est\u00e1 vivo <code>isAliveSpouse</code> Bin\u00e1rio Indica se o c\u00f4njuge do personagem est\u00e1 vivo <code>isMarried</code> Bin\u00e1rio Indica se o personagem \u00e9 casado <code>isNoble</code> Bin\u00e1rio Indica se o personagem \u00e9 nobre <code>age</code> Inteiro Idade do personagem (refer\u00eancia: ano 305 D.C.) <code>numDeadRelations</code> Inteiro N\u00famero de personagens mortos com os quais o personagem se relaciona <code>boolDeadRelations</code> Bin\u00e1rio Indica se h\u00e1 personagens mortos relacionados ao personagem <code>isPopular</code> Bin\u00e1rio Indica se o personagem \u00e9 considerado popular <code>popularity</code> Float \u00cdndice entre 0 e 1 que indica o qu\u00e3o popular \u00e9 o personagem <code>isAlive</code> Bin\u00e1rio Indica se o personagem est\u00e1 vivo"},{"location":"decision-tree/main/#estudo-da-coluna-plod","title":"Estudo da coluna <code>plod</code>","text":"<p>No dataset, temos uma coluna que possui um \u00edndice que aponta algo n\u00e3o identificado: o plod. Para investigar seu significado, s\u00e3o necess\u00e1rias algumas an\u00e1lises:</p> <ul> <li>Inspe\u00e7\u00e3o dos valores: Primeiro, foram realizadas algumas linhas de c\u00f3digo para verificar os valores da coluna;</li> </ul> Sa\u00eddaC\u00f3digo <p>Tipo de dado: float64</p> <p>Valor m\u00ednimo: 0.0</p> <p>Valor m\u00e1ximo: 1.0</p> <p>Valor m\u00e9dio: 0.366</p> <p>Exemplo de valor: 0.946</p> <pre><code>import pandas as pd\n\ndf = pd.read_csv(\"docs/decision-tree/dados.csv\", sep=\",\", encoding=\"UTF-8\")\n\nprint(f\"Tipo de dado: {df[\"plod\"].dtype}\\n\")\nprint(f\"Valor m\u00ednimo: {df[\"plod\"].min()}\\n\")\nprint(f\"Valor m\u00e1ximo: {df[\"plod\"].max()}\\n\")\nprint(f\"Valor m\u00e9dio: {format(df[\"plod\"].mean(), \".3f\")}\\n\")\nprint(f\"Exemplo de valor: {df.loc[0, \"plod\"]}\")\n</code></pre> <p>A an\u00e1lise da sa\u00edda obtida nos permite observar que os valores est\u00e3o sempre no intervalo [0,1], sugerindo que representam uma probabilidade ou \u00edndice normalizado.</p> <ul> <li>Correla\u00e7\u00f5es entre <code>plod</code> e as outras colunas: Levando isso em considera\u00e7\u00e3o, \u00e9 necess\u00e1rio realizar um c\u00e1lculo de correla\u00e7\u00f5es para descobrir a principal vari\u00e1vel no c\u00e1lculo do plod:</li> </ul> Sa\u00eddaC\u00f3digo <p>popularity: 0.35458415491153905</p> <p>isAliveFather: -0.3525990385007833</p> <p>book4: -0.4041512952984149</p> <p>isAlive: -0.41731839569897605</p> <pre><code>import pandas as pd\n\ndf = pd.read_csv(\"docs/decision-tree/dados.csv\", sep=\",\", encoding=\"UTF-8\")\n\ndf_numerico = df.select_dtypes(include=[\"number\"])\n\ncorrel = df_numerico.corr()[\"plod\"].sort_values(ascending=False)\n\nfor col, corr in correl.items():\n    if (corr &gt; 0.3 or corr &lt; -0.3) and corr != 1:\n        print(f\"{col}: {corr}\\n\")\n</code></pre> <p>\u00c9 poss\u00edvel observar que a correla\u00e7\u00e3o mais forte entre plod e qualquer outra coluna no dataset \u00e9 com a coluna isAlive. Esse dado nos permite criar uma hip\u00f3tese de que plod \u00e9 a estimativa da probabilidade de morte do personagem.</p> <ul> <li>Compara\u00e7\u00e3o com a coluna <code>isAlive</code>: Em seguida, para verificar a hip\u00f3tese estabelecida, ser\u00e1 feito um gr\u00e1fico de boxplot para analisar a rela\u00e7\u00e3o de plod e isAlive;</li> </ul> Gr\u00e1ficoC\u00f3digo 2025-10-29T22:00:15.971967 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <pre><code>import matplotlib.pyplot as plt\nimport pandas as pd\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/decision-tree/dados.csv\", sep=\",\", encoding=\"UTF-8\")\n\nplod_alive = df[df[\"isAlive\"] == 1][\"plod\"]\nplod_dead = df[df[\"isAlive\"] == 0][\"plod\"]\n\nplt.rcParams[\"figure.figsize\"] = (10, 5)\nfig, ax = plt.subplots(facecolor=\"white\")\nax.set_facecolor(\"white\")\n\nax.boxplot([plod_alive, plod_dead], tick_labels=[\"Vivo\", \"Morto\"],\n           patch_artist=True,\n           boxprops=dict(facecolor=\"lightblue\", color=\"black\"),\n           medianprops=dict(color=\"red\"))\n\nax.set_title(\"Distribui\u00e7\u00e3o de plod por estado de vida\", color=\"black\")\nax.set_ylabel(\"plod (probabilidade de morte)\", color=\"black\")\nax.set_xlabel(\"Estado de vida (isAlive)\", color=\"black\")\nax.grid(axis=\"y\", linestyle=\"--\", alpha=0.7, color=\"gray\")\n\nax.spines[\"top\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)\nax.spines[\"left\"].set_color(\"black\")\nax.spines[\"bottom\"].set_color(\"black\")\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> <p>No gr\u00e1fico, observa-se que personagens vivos (isAlive = 1) tendem a possuir baixos valores de plod, enquanto personagens mortos (isAlive = 0) geralmente t\u00eam valores altos. Al\u00e9m disso, \u00e9 poss\u00edvel observar diversos outliers dentre os personagens vivos, que s\u00e3o provavelmente personagens que aparecem pouco e/ou possuem informa\u00e7\u00f5es incompletas. Essa ideia \u00e9 fortalecida pelo fato de que a vari\u00e1vel popularity (popularidade) tamb\u00e9m possui correla\u00e7\u00e3o moderada com plod.</p> <p>Portanto, os padr\u00f5es do gr\u00e1fico indicam, novamente, que plod funciona como uma estimativa da probabilidade de morte do personagem, refor\u00e7ando a hip\u00f3tese inicial. Essa coluna provavelmente foi calculada com algum modelo preditivo anterior.</p> <p>\u00c9 necess\u00e1rio ressaltar que isso \u00e9 uma observa\u00e7\u00e3o explorat\u00f3ria, baseada nos dados dispon\u00edveis, e ser\u00e1 considerada no pr\u00e9-processamento e na escolha das features do modelo.</p>"},{"location":"decision-tree/main/#exploracao-aprofundada-da-coluna-popularity","title":"Explora\u00e7\u00e3o aprofundada da coluna <code>popularity</code>","text":"<ul> <li>Estat\u00edsticas descritivas: Primeiramente, vamos calcular alguns valores essenciais dessa coluna;</li> </ul> Sa\u00eddaC\u00f3digo popularity count 1946 mean 0.0895843 std 0.160568 min 0 25% 0.0133779 50% 0.0334448 75% 0.0869565 max 1 <pre><code>import pandas as pd\n\ndf = pd.read_csv(\"docs/decision-tree/dados.csv\", sep=\",\", encoding=\"UTF-8\")\n\nprint(df[\"popularity\"].describe().to_markdown())\n</code></pre> <p>Na sa\u00edda, observa-se que popularity \u00e9 um \u00edndice que indica a popularidade do personagem, variando entre 0 e 1, com o valor 0 para irrelevante e 1 para popular.</p> <ul> <li>Gr\u00e1fico de dispers\u00e3o de <code>popularity</code>: O gr\u00e1fico relaciona o \u00edndice popularity com a soma das 5 vari\u00e1veis book, que indicam a presen\u00e7a de um personagem em cada livro em bin\u00e1rio. Os livros considerados nessas vari\u00e1veis s\u00e3o apenas a narrativa principal da hist\u00f3ria, sem spin-offs e personagens que s\u00e3o apenas citados e referenciados.</li> </ul> Gr\u00e1ficoC\u00f3digo 2025-10-29T22:00:16.119530 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <pre><code>import matplotlib.pyplot as plt\nimport pandas as pd\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/decision-tree/dados.csv\", sep=\",\", encoding=\"UTF-8\")\n\ndf[\"book_freq\"] = df[[\"book1\", \"book2\", \"book3\", \"book4\", \"book5\"]].sum(axis=1)\n\nplt.rcParams[\"figure.figsize\"] = (10, 5)\nfig, ax = plt.subplots(facecolor=\"white\")\nax.set_facecolor(\"white\")\n\nax.scatter(df[\"book_freq\"], df[\"popularity\"], alpha=0.7, color=\"red\", edgecolor=\"black\")\n\nax.set_title(\"Popularidade X Frequ\u00eancia nos livros\", color=\"black\")\nax.set_xlabel(\"Frequ\u00eancia em livros (soma)\", color=\"black\")\nax.set_ylabel(\"Popularidade\", color=\"black\")\nax.grid(axis=\"y\", linestyle=\"--\", alpha=0.7, color=\"gray\")\n\nax.spines[\"top\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)\nax.spines[\"left\"].set_color(\"black\")\nax.spines[\"bottom\"].set_color(\"black\")\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> <p>A an\u00e1lise do gr\u00e1fico indica que, de 1 at\u00e9 5 apari\u00e7\u00f5es, o n\u00famero de personagens populares aumenta de forma diretamente proporcional, com alguns out-liers.</p> <p>Contudo, podemos observar que diversos personagens que n\u00e3o apareceram na s\u00e9rie principal de livros, possuindo soma de apari\u00e7\u00f5es igual a 0, s\u00e3o extremamente populares. Isso acontece pois h\u00e1 personagens de spin-offs muito amados pela comunidade, al\u00e9m de outros personagens que s\u00e3o apenas citados ao longo da hist\u00f3ria, sem aparecer diretamente, e tamb\u00e9m adquirem alta popularidade.</p>"},{"location":"decision-tree/main/#etapa-2-pre-processamento","title":"Etapa 2 - Pr\u00e9-processamento","text":"<p>O objetivo do projeto \u00e9 realizar uma predi\u00e7\u00e3o da relev\u00e2ncia dos personagens na trama principal, a vari\u00e1vel categ\u00f3rica relevance que possuir\u00e1 as seguintes categorias: Low, Medium, High e Very High. </p>"},{"location":"decision-tree/main/#1-passo-criacao-de-book_freq","title":"1\u00b0 Passo: Cria\u00e7\u00e3o de <code>book_freq</code>","text":"<p>Primeiramente, \u00e9 importante criar uma vari\u00e1vel representante para a frequ\u00eancia em livros para cada personagem. Ao inv\u00e9s de utilizar 5 vari\u00e1veis book diferentes, criaremos a vari\u00e1vel boof_freq. Para isso, ser\u00e1 feita a soma dos 5 valores das vari\u00e1veis book, o que resultar\u00e1 em um intervalo de [0,5]. Contudo, para que esse valores sejam normalizados, e possuam um n\u00famero entre 0 e 1, \u00e9 feita a divis\u00e3o desse resultado por 5.</p> <pre><code>df[\"book_freq\"] = df[[\"book1\", \"book2\", \"book3\", \"book4\",\"book5\"]].sum(axis=1) / 5\n</code></pre>"},{"location":"decision-tree/main/#2-passo-selecao-de-colunas","title":"2\u00b0 Passo: Sele\u00e7\u00e3o de colunas","text":"<p>Em seguida, \u00e9 necess\u00e1rio definir quais s\u00e3o as vari\u00e1veis ser\u00e3o utilizadas para prever a relev\u00e2ncia. Elas s\u00e3o as seguintes:</p> <ul> <li> <p>plod: Se esse valor for alto, h\u00e1 maior chance do personagem ser irrelevante </p> </li> <li> <p>title: Se o personagem tiver um t\u00edtulo, qualquer que seja, j\u00e1 possui alguma relev\u00e2ncia </p> </li> <li> <p>culture: Se o personagem tiver alguma cultura, qualquer que seja, j\u00e1 possui alguma relev\u00e2ncia </p> </li> <li> <p>mother: Se o personagem tiver paretentesco revelado, provavelmente tem alguma import\u00e2ncia </p> </li> <li> <p>father: Se o personagem tiver paretentesco revelado, provavelmente tem alguma import\u00e2ncia </p> </li> <li> <p>heir: Se o personagem tiver paretentesco revelado, provavelmente tem alguma import\u00e2ncia </p> </li> <li> <p>house: Se o personagem tiver uma casa, deve ser mais relevante </p> </li> <li> <p>book_freq: Se o personagem aparece frequentemente, deve ser relevante </p> </li> <li> <p>isNoble: Se o personagem for um nobre, tem mais chances de ser importante </p> </li> <li> <p>popularity: Se o personagem for popular, tamb\u00e9m aumenta sua chance de relev\u00e2ncia </p> </li> </ul> <p>A sele\u00e7\u00e3o das colunas foi feita, em c\u00f3digo, da seguinte forma: </p><pre><code>cols = [\"plod\", \"title\", \"culture\", \"mother\", \"father\", \"heir\", \"house\",\n\"book_freq\", \"isNoble\", \"popularity\"]\n\ndf = df[cols]\n</code></pre><p></p>"},{"location":"decision-tree/main/#3-passo-tratamento-de-valores-faltantes","title":"3\u00b0 Passo: Tratamento de valores faltantes","text":"<p>Precisamos garantir que n\u00e3o existam valores faltantes no dataframe. Por isso, ser\u00e1 feita uma altera\u00e7\u00e3o em todas as linhas restantes que possuem valor NA. Contudo, temos que tratar diferentemente cada tipo de vari\u00e1vel para o preenchimento dos vazios. As regras utilizadas ser\u00e3o as seguintes:</p> <ul> <li> <p>Faltantes n\u00famericos ser\u00e3o preenchidos com a mediana da coluna - plod, popularity</p> </li> <li> <p>Faltantes categ\u00f3ricos nominais ser\u00e3o preenchidos com \"Unknown\" (Desconhecido) - title, culture, mother, father, heir, house</p> </li> <li> <p>Faltantes bin\u00e1rios ser\u00e3o preenchidos com a moda da coluna (valor mais frequente) - isNoble</p> </li> </ul> <pre><code>cols = [\"plod\", \"popularity\"]\nfor col in cols:\n    df.fillna({col: df[col].median()}, inplace=True)\n\ncols = [\"title\", \"culture\", \"mother\", \"father\", \"heir\", \"house\"]\nfor col in cols:\n    df.fillna({col: \"Unknown\"}, inplace=True)\n\ndf.fillna({\"isNoble\": df[\"isNoble\"].mode()[0]}, inplace=True)\n</code></pre>"},{"location":"decision-tree/main/#4-passo-binarizacao-dos-categoricos-nominais","title":"4\u00b0 Passo: Binariza\u00e7\u00e3o dos categ\u00f3ricos nominais","text":"<p>As vari\u00e1veis categ\u00f3ricas no dataframe tem, simplesmente, muitas categorias para a realiza\u00e7\u00e3o de Label ou One-hot Encoding. Al\u00e9m disso, o \u00fanico dado importante provindo dessas no modelo sendo criado \u00e9 se existem ou n\u00e3o essas informa\u00e7\u00f5es sobre o personagem. Portanto, as colunas title, culture, mother, father, heir e house ser\u00e3o binarizadas. Ou seja, se possu\u00edrem um valor, assumir\u00e3o o valor 1. Caso contr\u00e1rio, 0. </p> <p>Al\u00e9m disso, os nomes das colunas ser\u00e3o alterados, adicionando um \"has_\" antes do nome original da vari\u00e1vel.</p> <pre><code>cols = [\"title\", \"culture\", \"mother\", \"father\", \"heir\", \"house\"]\n\nfor col in cols:\n    df[f\"has_{col}\"] = (df[col] != \"Unknown\").astype(int)\n    df.drop(columns=[col], inplace=True)\n</code></pre>"},{"location":"decision-tree/main/#5-passo-inversao-e-renomeacao-de-plod","title":"5\u00b0 Passo: Invers\u00e3o e renomea\u00e7\u00e3o de <code>plod</code>","text":"<p>A vari\u00e1vel plod, que indica a probabilidade de morte, possui uma rela\u00e7\u00e3o inversamente proporcional \u00e0 relev\u00e2ncia do personagem. Por isso, \u00e9 necess\u00e1ria a invers\u00e3o dessa vari\u00e1vel. Al\u00e9m disso, renomear a vari\u00e1vel para survival_prob deixar\u00e1 mais claro o seu prop\u00f3sito.</p> <pre><code>df[\"survival_prob\"] = 1 - df[\"plod\"]\ndf.drop(columns=\"plod\", inplace=True)\n</code></pre>"},{"location":"decision-tree/main/#6-passo-criacao-da-variavel-target-relevance_category-a-partir-do-score-relevance_score","title":"6\u00b0 Passo: Cria\u00e7\u00e3o da vari\u00e1vel target <code>relevance_category</code> a partir do score <code>relevance_score</code>","text":"<p>Agora, precisamos criar a vari\u00e1vel categ\u00f3rica que ser\u00e1 avaliada pelo modelo. Utilizaremos a seguinte distribui\u00e7\u00e3o de pesos:</p> <ul> <li> <p>popularity: Popularidade - 25%</p> </li> <li> <p>book_freq: Frequ\u00eancia de apari\u00e7\u00f5es - 25%</p> </li> <li> <p>survival_prob: Probabilidade de sobreviv\u00eancia - 15%</p> </li> <li> <p>isNoble: \u00c9 nobre - 10%</p> </li> <li> <p>has_title: Tem um t\u00edtulo - 10%</p> </li> <li> <p>has_house: Possui uma casa - 5%</p> </li> <li> <p>has_culture: Tem uma cultura - 5%</p> </li> <li> <p>has_mother + has_father + has_heir: Possui parentesco - 5%</p> </li> </ul> <p>Com o relevance_score definido, criaremos a nova coluna, relevance_category, a partir dos seguintes valores de score:</p> <ul> <li> <p>x &lt; 0.25: Low (Baixa relev\u00e2ncia)</p> </li> <li> <p>0.25 &lt;= x &lt; 0.5: Medium (Relev\u00e2ncia m\u00e9dia)</p> </li> <li> <p>0.5 &lt;= x &lt; 0.75: High (Alta relev\u00e2ncia)</p> </li> <li> <p>0.75 &lt;= x &lt;= 1: Very High (Relev\u00e2ncia muito alta)</p> </li> </ul>"},{"location":"decision-tree/main/#resultado-final-do-pre-processamento","title":"Resultado final do pr\u00e9-processamento","text":"Sa\u00eddaC\u00f3digo <p>Colunas ap\u00f3s tratamento: ['book_freq', 'isNoble', 'popularity', 'has_title', 'has_culture', 'has_mother', 'has_father', 'has_heir', 'has_house', 'survival_prob', 'relevance_score', 'relevance_category']</p> <p>Valores ausentes ap\u00f3s pr\u00e9-processamento: 0</p> <p>Formato do dataset final: (1946, 12)</p> <p>Features: 10</p> <p>Target: relevance_category</p> <p>Distribui\u00e7\u00e3o da vari\u00e1vel target:</p> relevance_category count Medium 1078 Low 458 High 379 Very High 31 <pre><code>import pandas as pd\n\ndf = pd.read_csv(\"docs/decision-tree/dados.csv\", sep=\",\", encoding=\"UTF-8\")\n\n# 1\u00b0 Passo: Criando a coluna \"book_freq\", j\u00e1 normalizando-a\n\ndf[\"book_freq\"] = df[[\"book1\", \"book2\", \"book3\", \"book4\", \"book5\"]].sum(axis=1) / 5\n\n# 2\u00b0 Passo: Dropando colunas que n\u00e3o ser\u00e3o utilizadas\n\ncols = [\n    \"plod\", \"title\", \"culture\", \"mother\", \"father\", \"heir\", \"house\", \"book_freq\", \"isNoble\", \"popularity\"\n]\n\ndf = df[cols]\n\n# 3\u00b0 Passo: Tratamento de valores faltantes\n\ncols = [\"plod\", \"popularity\"]\nfor col in cols:\n    df.fillna({col: df[col].median()}, inplace=True)\n\ncols = [\"title\", \"culture\", \"mother\", \"father\", \"heir\", \"house\"]\nfor col in cols:\n    df.fillna({col: \"Unknown\"}, inplace=True)\n\ndf.fillna({\"isNoble\": df[\"isNoble\"].mode()[0]}, inplace=True)\n\n# 4\u00b0 Passo: Binariza\u00e7\u00e3o das vari\u00e1veis categ\u00f3ricas nominais\n\ncols = [\"title\", \"culture\", \"mother\", \"father\", \"heir\", \"house\"]\n\nfor col in cols:\n    df[f\"has_{col}\"] = (df[col] != \"Unknown\").astype(int)\n    df.drop(columns=[col], inplace=True)\n\n# 5\u00b0 Passo: Invers\u00e3o da vari\u00e1vel \"plod\" e renomea\u00e7\u00e3o para \"survival_prob\"\n\ndf[\"survival_prob\"] = 1 - df[\"plod\"]\ndf.drop(columns=\"plod\", inplace=True)\n\n# 6\u00b0 Passo: Criar vari\u00e1vel target \"relevance_category\" a partir do score \"relevance_score\"\n\ndef calculate_relevance_score(row):\n\n    score = (\n        row[\"popularity\"] * 0.25 +\n        row[\"book_freq\"] * 0.25 +\n        row[\"survival_prob\"] * 0.15 +\n        row[\"isNoble\"] * 0.10 +\n        row[\"has_title\"] * 0.10 +\n        row[\"has_house\"] * 0.05 +\n        row[\"has_culture\"] * 0.05 +\n        (row[\"has_mother\"] + row[\"has_father\"] + row[\"has_heir\"]) * 0.05 / 3\n    )\n\n    return min(max(score, 0), 1)\n\ndef categorize_relevance(score):\n    if score &lt; 0.25:\n        return \"Low\"\n    elif score &lt; 0.5:\n        return \"Medium\"\n    elif score &lt; 0.75:\n        return \"High\"\n    else:\n        return \"Very High\"\n\ndf[\"relevance_score\"] = df.apply(calculate_relevance_score, axis=1)\ndf[\"relevance_category\"] = df[\"relevance_score\"].apply(categorize_relevance)\n\nfeatures = [\n    \"book_freq\", \"popularity\", \"survival_prob\", \"isNoble\",    \n    \"has_title\", \"has_culture\", \"has_mother\", \"has_father\", \n    \"has_heir\", \"has_house\",\n]\n\ntarget = \"relevance_category\"\n\n# df.to_csv(\"dados_processado.csv\", index=False)\n\nprint(f\"Colunas ap\u00f3s tratamento: {df.columns.tolist()}\\n\")\nprint(f\"Valores ausentes ap\u00f3s pr\u00e9-processamento: {df.isnull().sum().sum()}\\n\") \nprint(f\"Formato do dataset final: {df.shape}\\n\")\nprint(f\"Features: {len(features)}\\n\")\nprint(f\"Target: {target}\\n\")\nprint(f\"Distribui\u00e7\u00e3o da vari\u00e1vel target:\\n\")\nprint(df[target].value_counts().to_markdown())\n</code></pre>"},{"location":"decision-tree/main/#etapa-3-divisao-de-dados","title":"Etapa 3 - Divis\u00e3o de dados","text":"<p>Na etapa de divis\u00e3o de dados, separaremos o conjunto de dados processado em dois grupos distintos:</p> <ul> <li> <p>Conjunto de Treino: \u00c9 utilizado para ensinar o modelo a reconhecer padr\u00f5es</p> </li> <li> <p>Conjunto de Teste: \u00c9 utilizado para avaliar o desempenho do modelo com dados ainda n\u00e3o vistos</p> </li> </ul> <p>Para realizar a divis\u00e3o, utilizaremos a fun\u00e7\u00e3o <code>train_test_split()</code> do <code>scikit-learn</code>. Os par\u00e2metros utilizados ser\u00e3o:</p> <ul> <li> <p>test_size=0.2: Define que 20% dos dados ser\u00e3o utilizados para teste, enquanto o restante ser\u00e1 usado para treino.</p> </li> <li> <p>random_state=42: Par\u00e2metro que controla o gerador de n\u00famero aleat\u00f3rios utilizado para sortear os dados antes de separ\u00e1-los. Garante reprodutibilidade.</p> </li> <li> <p>stratify=y: Esse atributo definido como y \u00e9 essencial devido \u00e0 natureza da coluna relevance_category. Com essa defini\u00e7\u00e3o, ser\u00e1 mantida a mesma propor\u00e7\u00e3o das categorias em ambos os conjuntos, reduzindo o vi\u00e9s.</p> </li> </ul> Sa\u00eddaC\u00f3digo <p>Treino: 1556 amostras</p> <p>Teste: 390 amostras</p> <p>Propor\u00e7\u00e3o: 80.0% treino, 20.0% teste</p> <p>Distribui\u00e7\u00e3o das classes - </p> <p>Treino:</p> relevance_category count Medium 862 Low 366 High 303 Very High 25 <p>Teste:</p> relevance_category count Medium 216 Low 92 High 76 Very High 6 <pre><code>import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndf = pd.read_csv(\"docs/decision-tree/dados_processado.csv\")\n\nfeatures = [\n    \"book_freq\", \"popularity\", \"survival_prob\", \"isNoble\",\n    \"has_title\", \"has_culture\", \"has_mother\", \"has_father\", \n    \"has_heir\", \"has_house\"\n]\n\ntarget = \"relevance_category\"\n\nX = df[features]\ny = df[target]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\nprint(f\"Treino: {X_train.shape[0]} amostras\\n\")\nprint(f\"Teste: {X_test.shape[0]} amostras\\n\")\nprint(f\"Propor\u00e7\u00e3o: {X_train.shape[0]/X.shape[0]*100:.1f}% treino, {X_test.shape[0]/X.shape[0]*100:.1f}% teste\\n\")\n\nprint(\"Distribui\u00e7\u00e3o das classes - \\n\")\nprint(\"Treino:\\n\")\nprint(y_train.value_counts().to_markdown(), \"\\n\")\nprint(\"Teste:\\n\")\nprint(y_test.value_counts().to_markdown(), \"\\n\")\n</code></pre> <p>Os dados, agora, est\u00e3o devidamente divididos. Esta divis\u00e3o adequada \u00e9 de extrema import\u00e2ncia, pois ajuda a evitar overfitting e garante que o modelo possa generalizar bem para novos personagens n\u00e3o vistos durante o treinamento.</p>"},{"location":"decision-tree/main/#etapa-4-treinamento-do-modelo","title":"Etapa 4 - Treinamento do Modelo","text":"<p>Agora, ser\u00e1 realizado o treinamento do modelo. O objetivo dessa etapa \u00e9 ensinar o algoritmo a reconhecer padr\u00f5es nos dados que s\u00e3o fornecidos, e determinar a import\u00e2ncia narrativa de cada personagem na s\u00e9rie principal de livros de A Song of Ice and Fire.</p> \u00c1rvoreC\u00f3digo <p>Precis\u00e3o do Modelo: 0.9513 Import\u00e2ncia das Features:  Feature Import\u00e2ncia 0 book_freq 0.439754 2 survival_prob 0.200915 3 isNoble 0.146227 1 popularity 0.102306 5 has_culture 0.054061 9 has_house 0.034825 4 has_title 0.021911 6 has_mother 0.000000 7 has_father 0.000000 8 has_heir 0.000000 2025-10-29T22:00:17.232946 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ </p> <pre><code>import matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn import tree\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/decision-tree/dados_processado.csv\")\n\nfeatures = [\n    \"book_freq\", \"popularity\", \"survival_prob\", \"isNoble\",\n    \"has_title\", \"has_culture\", \"has_mother\", \"has_father\", \n    \"has_heir\", \"has_house\"\n]\n\ntarget = \"relevance_category\"\n\nX = df[features]\ny = df[target]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\nclassifier = tree.DecisionTreeClassifier(random_state=42)\nclassifier.fit(X_train, y_train)\n\ny_pred = classifier.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Precis\u00e3o do Modelo: {accuracy:.4f}\")\n\nfeature_importance = pd.DataFrame({\n    \"Feature\": classifier.feature_names_in_,\n    \"Import\u00e2ncia\": classifier.feature_importances_\n})\nprint(\"&lt;br&gt;Import\u00e2ncia das Features:\")\nprint(feature_importance.sort_values(by=\"Import\u00e2ncia\", ascending=False).to_html() + \"&lt;br&gt;\")\n\nplt.figure(figsize=(20, 10))\ntree.plot_tree(\n    classifier, \n    feature_names=features,\n    class_names=classifier.classes_,\n    filled=True,\n    rounded=True,\n    max_depth=3, \n    fontsize=10\n)\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\")\nprint(buffer.getvalue())\nplt.close()\n</code></pre>"},{"location":"decision-tree/main/#etapa-5-avaliacao-do-modelo","title":"Etapa 5 - Avalia\u00e7\u00e3o do modelo","text":""},{"location":"decision-tree/main/#acuracia-do-modelo","title":"Acur\u00e1cia do modelo","text":"<p>O modelo alcan\u00e7ou uma acur\u00e1cia impressionante de 95,13% no conjunto teste, demonstrando uma \u00f3tima capacidade de previs\u00e3o com personagens ainda n\u00e3o vistos com base nas features escolhidas.</p>"},{"location":"decision-tree/main/#importancia-das-features","title":"Import\u00e2ncia das features","text":"<p>A an\u00e1lise da import\u00e2ncia das features revela quais foram as vari\u00e1veis mais importantes para a previs\u00e3o e decis\u00f5es do modelo:</p> Feature Import\u00e2ncia Descri\u00e7\u00e3o <code>book_freq</code> 43,98% Frequ\u00eancia de apari\u00e7\u00e3o nos livros da s\u00e9rie principal <code>survival_prob</code> 20,09% Probabilidade de sobreviv\u00eancia <code>isNoble</code> 14,62% Status nobre <code>popularity</code> 10,23% \u00cdndice de popularidade <code>has_culture</code> 5,41% Possui cultura conhecida <code>has_house</code> 3,48% Pertence a uma casa <code>has_title</code> 2,19% Tem algum t\u00edtulo <code>has_mother</code> 0,00% H\u00e1 informa\u00e7\u00e3o sobre a m\u00e3e <code>has_father</code> 0,00% H\u00e1 informa\u00e7\u00e3o sobre o pai <code>has_heir</code> 0,00% Possui herdeiro"},{"location":"decision-tree/main/#insights-importantes-sobre-o-modelo","title":"Insights importantes sobre o modelo","text":"<ul> <li> <p>Frequ\u00eancia em livros \u00e9 determinante: A feature <code>book_freq</code> responde \u00e0 aproximadamente 44% da import\u00e2ncia, confirmando que personagens com mais apari\u00e7\u00f5es em diferentes livros da s\u00e9rie principal possuem maior import\u00e2ncia.</p> </li> <li> <p>Features desnecess\u00e1rias: O modelo tamb\u00e9m demonstrou que algumas features (has_mother, has_father, has_heir) n\u00e3o t\u00eam nenhuma import\u00e2ncia na predi\u00e7\u00e3o.</p> </li> </ul>"},{"location":"decision-tree/main/#etapa-6-relatorio-final","title":"Etapa 6 - Relat\u00f3rio Final","text":"<p>O projeto geral foi um sucesso, com a obten\u00e7\u00e3o de um modelo com uma acur\u00e1cia de 95,13%. O modelo, al\u00e9m de alta performance, possui features relevantes identificadas e bem estabelecidas: <code>book_freq</code>, <code>survival_prob</code> e <code>isNoble</code>.</p>"},{"location":"decision-tree/main/#limitacoes-do-modelo","title":"Limita\u00e7\u00f5es do modelo","text":"<p>Contudo, h\u00e1 limita\u00e7\u00f5es no modelo:</p> <ul> <li> <p>Features Redudantes: has_mother, has_father e has_heir possuem import\u00e2ncia nula para a predi\u00e7\u00e3o do sistema</p> </li> <li> <p>Poss\u00edvel vi\u00e9s: \u00c9 poss\u00edvel que, pela vari\u00e1vel relevance_score ter sido manualmente estabelecida, pode haver vi\u00e9s</p> </li> </ul>"},{"location":"decision-tree/main/#possiveis-melhorias","title":"Poss\u00edveis melhorias","text":"<ul> <li> <p>Valida\u00e7\u00e3o de <code>plod</code>: Durante a primeira etapa, na explora\u00e7\u00e3o da base de dados, poderia ter sido feita uma Regress\u00e3o Linear M\u00faltipla completa para validar completamente a hip\u00f3tese de que plod \u00e9 a probabilidade de morte do personagem.</p> </li> <li> <p>Remo\u00e7\u00e3o de features desnecess\u00e1rias: As features relacionadas \u00e0 parentesco podem ser removidas do modelo sem nenhum impacto na predi\u00e7\u00e3o.</p> </li> </ul>"},{"location":"decision-tree/main/#consideracoes-finais","title":"Considera\u00e7\u00f5es finais","text":"<p>A \u00e1rvore de decis\u00e3o se mostrou muito capaz de fazer a predi\u00e7\u00e3o de narrativas liter\u00e1rias complexas como A Song of Ice and Fire. Al\u00e9m do excelente resultado de acur\u00e1cia, foram providos insights importantes sobre a obra pelo modelo. </p> <p>Al\u00e9m disso, foi poss\u00edvel observar que tanto a Etapa 1 quanto a Etapa 2 foram muito mais longas do que as posteriores, demonstrando a import\u00e2ncia de entender e limpar o dataset antes do uso.</p>"},{"location":"k-means/main/","title":"Projeto 3 - K-Means","text":""},{"location":"k-means/main/#modelo-de-machine-learning-k-means","title":"Modelo de Machine Learning - K-Means","text":"<p>Para esse projeto, foi utilizado um dataset obtido no Kaggle. Os dados usados podem ser baixados aqui.</p>"},{"location":"k-means/main/#objetivo","title":"Objetivo","text":"<p>O dataset utilizado possui dados sint\u00e9ticamente gerados sobre peixes inspirados no papel Length-weight relationships of nine \ufb01sh species from the Tetulia River, southern Bangladesh. O objetivo do projeto \u00e9 clusterizar esses peixes atrav\u00e9s, puramente, de suas caracter\u00edsticas f\u00edsicas.</p>"},{"location":"k-means/main/#workflow","title":"Workflow","text":"<p>Os pontos \"etapas\" s\u00e3o o passo-a-passo da realiza\u00e7\u00e3o do projeto.</p>"},{"location":"k-means/main/#etapa-1-exploracao-de-dados","title":"Etapa 1 - Explora\u00e7\u00e3o de Dados","text":"<p>Primeiramente, deve ser feita a explora\u00e7\u00e3o dos dados da base, com o objetivo de compreender a forma como s\u00e3o estruturados os dados e sua natureza.</p> <p>O dataset \u00e9 composto por 4080 linhas e 4 colunas, com cada linha representando uma peixe diferente. Essa verifica\u00e7\u00e3o p\u00f4de ser feita com as linhas de c\u00f3digo abaixo;</p> Sa\u00eddaC\u00f3digo <p>(4080, 4)</p> <pre><code>import pandas as pd\n\ndf = pd.read_csv(\"docs/k-means/fish_data.csv\")\n\nprint(df.shape)\n</code></pre>"},{"location":"k-means/main/#colunas-do-dataset","title":"Colunas do dataset","text":"Coluna Tipo Descri\u00e7\u00e3o species String Nome da esp\u00e9cie do peixe length Float Comprimento do peixe em cent\u00edmetros (cm) weight Float Peso do peixe em gramas (g) w_l_ratio Float Raz\u00e3o entre peso e comprimento (weight / length)"},{"location":"k-means/main/#visualizacoes-das-variaveis","title":"Visualiza\u00e7\u00f5es das vari\u00e1veis","text":"<p>Em seguida, \u00e9 essencial realizar gr\u00e1ficos para visualizar como cada uma das vari\u00e1veis se comportam, com o objetivo de entender melhor a base da dados.</p>"},{"location":"k-means/main/#variavel-species-categorica","title":"Vari\u00e1vel <code>species</code> - Categ\u00f3rica","text":"Gr\u00e1ficoC\u00f3digo 2025-10-29T22:00:17.645169 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/k-means/fish_data.csv\")\n\nplt.figure(figsize=(10, 8))\ndf[\"species\"].value_counts().plot(kind=\"bar\")\nplt.title(\"Distribui\u00e7\u00e3o das Esp\u00e9cies de Peixe\")\nplt.xlabel(\"Esp\u00e9cie\")\nplt.ylabel(\"Quantidade\")\nplt.xticks(rotation=45)\n\nax = plt.gca()\nfor p in ax.patches:\n    ax.annotate(f\"{int(p.get_height())}\", \n                (p.get_x() + p.get_width()/2., p.get_height()),\n                ha=\"center\", va=\"bottom\")\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> <p>\u00c9 poss\u00edvel observar que h\u00e1 9 esp\u00e9cies de peixe distintas, com uma distribui\u00e7\u00e3o muito semelhante entre elas. Com isso, podemos criar a hip\u00f3tese de que essa base \u00e9 fortemente compat\u00edvel com clusteriza\u00e7\u00e3o de dados.</p>"},{"location":"k-means/main/#variavel-length-quantitativa-continua","title":"Vari\u00e1vel <code>length</code> - Quantitativa Cont\u00ednua","text":"Gr\u00e1ficoC\u00f3digo 2025-10-29T22:00:17.887877 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/k-means/fish_data.csv\")\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\nax1.hist(df[\"length\"], bins=20, edgecolor=\"black\", alpha=0.7)\nax1.set_title(\"Distribui\u00e7\u00e3o do Comprimento\")\nax1.set_xlabel(\"Comprimento (cm)\")\nax1.set_ylabel(\"Frequ\u00eancia\")\n\nbplot = ax2.boxplot(df[\"length\"], patch_artist=True)\nax2.set_title(\"Boxplot - Comprimento\")\nax2.set_ylabel(\"Comprimento (cm)\")\n\ncolors = [\"peachpuff\"]\n\nfor patch, color in zip(bplot[\"boxes\"], colors):\n    patch.set_facecolor(color)\n\nplt.tight_layout()\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre>"},{"location":"k-means/main/#variavel-weight-quantitativa-continua","title":"Vari\u00e1vel <code>weight</code> - Quantitativa Cont\u00ednua","text":"Gr\u00e1ficoC\u00f3digo 2025-10-29T22:00:18.116551 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/k-means/fish_data.csv\")\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\nax1.hist(df[\"weight\"], bins=20, edgecolor=\"black\", alpha=0.7)\nax1.set_title(\"Distribui\u00e7\u00e3o do Peso\")\nax1.set_xlabel(\"Peso (g)\")\nax1.set_ylabel(\"Frequ\u00eancia\")\n\nbplot = ax2.boxplot(df[\"weight\"], patch_artist=True)\nax2.set_title(\"Boxplot - Peso\")\nax2.set_ylabel(\"Peso (g)\")\n\ncolors = [\"lightblue\"]\n\nfor patch, color in zip(bplot[\"boxes\"], colors):\n    patch.set_facecolor(color)\n\nplt.tight_layout()\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre>"},{"location":"k-means/main/#variavel-w_l_ratio-quantitativa-continua","title":"Vari\u00e1vel <code>w_l_ratio</code> - Quantitativa Cont\u00ednua","text":"Gr\u00e1ficoC\u00f3digo 2025-10-29T22:00:18.352315 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/k-means/fish_data.csv\")\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\nax1.hist(df[\"w_l_ratio\"], bins=20, edgecolor=\"black\", alpha=0.7)\nax1.set_title(\"Distribui\u00e7\u00e3o da Raz\u00e3o Peso/Comprimento\")\nax1.set_xlabel(\"Raz\u00e3o Peso/Comprimento\")\nax1.set_ylabel(\"Frequ\u00eancia\")\n\nbplot = ax2.boxplot(df[\"w_l_ratio\"], patch_artist=True)\nax2.set_title(\"Boxplot - Raz\u00e3o Peso/Comprimento\")\nax2.set_ylabel(\"Raz\u00e3o\")\n\ncolors = [\"lightgreen\"]\n\nfor patch, color in zip(bplot[\"boxes\"], colors):\n    patch.set_facecolor(color)\n\nplt.tight_layout()\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> <p>Com todas as an\u00e1lises realizadas, \u00e9 poss\u00edvel entender os dados, suas frequ\u00eancias e distribui\u00e7\u00f5es. Com isso, podemos partir para o pr\u00f3ximo passo.</p>"},{"location":"k-means/main/#etapa-2-pre-processamento","title":"Etapa 2 - Pr\u00e9-processamento","text":"<p>Nesta etapa, vamos fazer o pr\u00e9-processamento dos dados. Para o K-Means, por ser um modelo n\u00e3o supervisionado de machine learning, n\u00e3o h\u00e1 necessidade de realizar a divis\u00e3o de dados. Portanto, vamos aplicar o pr\u00e9-processamento em toda a base.</p>"},{"location":"k-means/main/#1-passo-remocao-da-coluna-species","title":"1\u00b0 Passo: Remo\u00e7\u00e3o da coluna <code>species</code>","text":"<p>A coluna <code>species</code> da base j\u00e1 possui a divis\u00e3o de cada um dos peixes por esp\u00e9cie. Portanto, vamos remover essa coluna e realizar a clusteriza\u00e7\u00e3o com os outros dados.</p> <pre><code>df = df.drop(columns=[\"species\"])\n</code></pre>"},{"location":"k-means/main/#2-passo-identificacao-e-tratamento-de-valores-nulos","title":"2\u00b0 Passo: Identifica\u00e7\u00e3o e tratamento de valores nulos","text":"<p>O primeiro passo para o pr\u00e9-processamento \u00e9 identificar e tratar valores nulos na base.</p> <pre><code>print(df.isna().sum())\n</code></pre> <p>Executando a linha de c\u00f3digo acima para o dataframe contendo os dados da base, foi poss\u00edvel identificar que n\u00e3o h\u00e1 valores nulos na base.</p>"},{"location":"k-means/main/#3-passo-padronizacao-dos-dados","title":"3\u00b0 Passo: Padroniza\u00e7\u00e3o dos dados","text":"<p>Em seguida, \u00e9 necess\u00e1ria a padroniza\u00e7\u00e3o das features num\u00e9ricas na base. Ao inv\u00e9s da normaliza\u00e7\u00e3o, ser\u00e1 utilizada a t\u00e9cnica de padroniza\u00e7\u00e3o devido aos outliers nas features num\u00e9ricas, principalmente as vari\u00e1veis <code>weight</code> e <code>w_l_ratio</code>. Para a padroniza\u00e7\u00e3o, utilizaremos o StandardScaler() do <code>scikit-learn</code>.</p> <pre><code>from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\ndf = df.drop(columns=[\"species\"])\n\nX = scaler.fit_transform(df)\n</code></pre> <p>Como todas as features restantes ap\u00f3s a remo\u00e7\u00e3o de <code>species</code> s\u00e3o num\u00e9ricas cont\u00ednuas, podemos simplesmente aplicar a padroniza\u00e7\u00e3o na base inteira de uma vez.</p>"},{"location":"k-means/main/#etapa-3-treinamento-do-modelo","title":"Etapa 3 - Treinamento do Modelo","text":"<p>Agora, ser\u00e1 realizado o treinamento do modelo. O objetivo dessa etapa \u00e9 clusterizar os dados e inserir os peixes em seus grupos a partir da t\u00e9cnica de K-Means.</p>"},{"location":"k-means/main/#elbow-method","title":"Elbow Method","text":"<p>Antes de treinar o modelo, \u00e9 necess\u00e1rio descobrir o n\u00famero de clusters que ser\u00e1 utilizado. Para isso, aplicaremos o Elbow Method.</p> ElbowC\u00f3digo <p></p> <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\ndf = pd.read_csv(\"docs/k-means/fish_data.csv\")\n\nX = df.drop(columns=[\"species\"])\n\nX = scaler.fit_transform(X)\n\n# Elbow Method\n\nwcss = []\nk_range = range(1, 11)\n\nfor k in k_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    kmeans.fit(X)\n    wcss.append(kmeans.inertia_)\n\nplt.figure(figsize=(10, 6))\nplt.plot(k_range, wcss, \"bo-\", markersize=8, linewidth=2)\nplt.xlabel(\"N\u00famero de Clusters (K)\")\nplt.ylabel(\"WCSS (Within-Cluster Sum of Square)\")\nplt.title(\"Elbow Method - Determinando o K ideal\")\nplt.grid(True, alpha=0.3)\nplt.xticks(k_range)\n\nplt.axvline(x=3, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"Poss\u00edvel cotovelo K=3\")\n\nplt.legend()\nplt.savefig(\"docs/images/elbow.svg\", format=\"svg\", transparent=True)\nplt.close()\n</code></pre> <p>Podemos observar que, muito provavelmente, o cotovelo est\u00e1 em \\(k = 3\\). Mesmo que saibamos que existem 9 esp\u00e9cies de peixes distintas na base, o Elbow Method nos mostra que os dados formam 3 grupos naturais baseados apenas nas medidas f\u00edsicas, n\u00e3o nas esp\u00e9cies biol\u00f3gicas. For\u00e7ar um cluster para cada esp\u00e9cie, fazendo de \\(k = 9\\), provavelmente resultar\u00e1 em overfitting e clusters ruins, j\u00e1 que as medidas podem n\u00e3o separar perfeitamente as 9 esp\u00e9cies.</p>"},{"location":"k-means/main/#resultado-do-treinamento","title":"Resultado do treinamento","text":"K-MeansC\u00f3digo Silhouette Score: 0.6284 <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import silhouette_score\n\nscaler = StandardScaler()\n\ndf = pd.read_csv(\"docs/k-means/fish_data.csv\")\n\nX = df.drop(columns=[\"species\"])\n\nX = scaler.fit_transform(X)\n\nkmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\ncluster_labels = kmeans.fit_predict(X)\n\nsilhouette_avg = silhouette_score(X, cluster_labels)\nprint(f\"Silhouette Score: {silhouette_avg:.4f}\")\n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\nplt.figure(figsize=(10, 8))\nscatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, cmap=\"viridis\", alpha=0.7)\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], marker=\"*\", s=300, c=\"red\", label=\"Centroids\")\n\nplt.xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]:.2%} var.)\")\nplt.ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]:.2%} var.)\")\nplt.title(\"Clusters de Peixes - K-means (K=3)\")\nplt.legend()\nplt.colorbar(scatter, label=\"Cluster\")\n\nplt.savefig(\"docs/images/k-means.svg\", format=\"svg\", transparent=True)\nplt.close()\n</code></pre>"},{"location":"k-means/main/#etapa-4-avaliacao-do-modelo","title":"Etapa 4 - Avalia\u00e7\u00e3o do modelo","text":""},{"location":"k-means/main/#silhouette-score","title":"Silhouette Score","text":"<p>O modelo alcan\u00e7ou um Silhouette Score de 0.6284, indicando uma estrutura de clusters bem definida e distinta. Na escala de -1 a +1, este valor se enquadra na categoria Boa a Forte, sugerindo que os clusters possuem alta coes\u00e3o interna e boa separa\u00e7\u00e3o entre si, com sobreposi\u00e7\u00e3o m\u00ednima entre os grupos.</p>"},{"location":"k-means/main/#variancia-explicada","title":"Vari\u00e2ncia Explicada","text":"<p>O PCA aplicado para visualiza\u00e7\u00e3o explica 98.80% da vari\u00e2ncia total dos dados, com o primeiro componente (PC1) capturando 58.68% e o segundo componente (PC2) 40.12%. Isso indica que a visualiza\u00e7\u00e3o 2D representa fielmente a estrutura multidimensional original dos dados de medidas dos peixes.</p>"},{"location":"k-means/main/#insights-importantes-sobre-o-modelo","title":"Insights Importantes sobre o Modelo","text":""},{"location":"k-means/main/#padrao-de-agrupamento-natural","title":"Padr\u00e3o de Agrupamento Natural","text":"<ul> <li> <p>O K-means identificou 3 grupos naturais baseados apenas nas medidas f\u00edsicas, n\u00e3o nas esp\u00e9cies biol\u00f3gicas</p> </li> <li> <p>Isso revela que diferentes esp\u00e9cies compartilham perfis dimensionais similares</p> </li> </ul>"},{"location":"k-means/main/#relacao-com-especies-reais","title":"Rela\u00e7\u00e3o com Esp\u00e9cies Reais","text":"<p>A an\u00e1lise de correspond\u00eancia mostra que:</p> <ul> <li> <p>Cada cluster cont\u00e9m m\u00faltiplas esp\u00e9cies</p> </li> <li> <p>Esp\u00e9cies diferentes coexistem no mesmo cluster quando t\u00eam medidas similares</p> </li> </ul>"},{"location":"k-means/main/#etapa-5-relatorio-final","title":"Etapa 5 - Relat\u00f3rio Final","text":"<p>O projeto de clustering foi bem-sucedido em identificar padr\u00f5es naturais nas medidas f\u00edsicas de peixes. O K-means com K=3 demonstrou ser a configura\u00e7\u00e3o ideal para estes dados.</p>"},{"location":"k-means/main/#possiveis-melhorias","title":"Poss\u00edveis Melhorias","text":"<ul> <li> <p>An\u00e1lise discriminante: Verificar se as esp\u00e9cies s\u00e3o linearmente separ\u00e1veis</p> </li> <li> <p>Matriz de confus\u00e3o clusters\u00d7esp\u00e9cies: Quantificar a sobreposi\u00e7\u00e3o entre agrupamentos</p> </li> <li> <p>An\u00e1lise de features: Identificar quais medidas mais contribuem para cada cluster</p> </li> </ul>"},{"location":"k-means/main/#consideracoes-finais","title":"Considera\u00e7\u00f5es Finais","text":"<p>O modelo K-means demonstrou ser eficaz para identificar padr\u00f5es de tamanho em peixes baseado em suas medidas f\u00edsicas. A descoberta mais interessante foi que o agrupamento natural dos dados (3 clusters) n\u00e3o corresponde \u00e0s 9 esp\u00e9cies biol\u00f3gicas, revelando que diferentes esp\u00e9cies podem ter perfis dimensionais similares.</p> <p>O Silhouette Score de 0.6284 confirma uma estrutura de clusters bem definida, com boa separa\u00e7\u00e3o entre grupos e alta coes\u00e3o interna.</p>"},{"location":"knn/main/","title":"Projeto 2 - KNN","text":""},{"location":"knn/main/#modelo-de-machine-learning-knn","title":"Modelo de Machine Learning - KNN","text":"<p>Para esse projeto, foi utilizado um dataset obtido no Kaggle. Os dados usados podem ser baixados aqui.</p>"},{"location":"knn/main/#objetivo","title":"Objetivo","text":"<p>O dataset utilizado possui informa\u00e7\u00f5es sobre reservas em um hotel, e foi criado justamente para a cria\u00e7\u00e3o de modelos de machine learning com o objetivo de prever se um agendamento ser\u00e1, ou n\u00e3o, cancelado.</p> <p>Os dados originais para a cria\u00e7\u00e3o desse dataset foram obtidos em um artigo de dados, no site Science Direct. O artigo em quest\u00e3o foi escrito por Nuno Antonio, Ana de Almeida e Luis Nunes, e cont\u00e9m uma quantidade maior de dados do que a sua vers\u00e3o derivada do Kaggle, que estou utilizando para este projeto.</p>"},{"location":"knn/main/#workflow","title":"Workflow","text":"<p>Os pontos \"etapas\" s\u00e3o o passo-a-passo da realiza\u00e7\u00e3o do projeto.</p>"},{"location":"knn/main/#etapa-1-exploracao-de-dados","title":"Etapa 1 - Explora\u00e7\u00e3o de Dados","text":"<p>Primeiramente, deve ser feita a explora\u00e7\u00e3o dos dados da base, com o objetivo de compreender a forma como s\u00e3o estruturados os dados, sua natureza e poss\u00edvel signific\u00e2ncia para o modelo de predi\u00e7\u00e3o.</p> <p>O dataset \u00e9 composto por 36285 linhas e 17 colunas, com cada linha representando uma reserva distinta. Essa verifica\u00e7\u00e3o p\u00f4de ser feita com as linhas de c\u00f3digo abaixo;</p> Sa\u00eddaC\u00f3digo <p>(36285, 17)</p> <pre><code>import pandas as pd\n\ndf = pd.read_csv(\"docs/knn/booking.csv\")\n\nprint(df.shape)\n</code></pre>"},{"location":"knn/main/#colunas-do-dataset","title":"Colunas do dataset","text":"Coluna Tipo Descri\u00e7\u00e3o Booking_ID String Identificador \u00fanico da reserva number of adults Inteiro N\u00famero de adultos presentes na reserva number of children Inteiro N\u00famero de crian\u00e7as presentes na reserva number of weekend nights Inteiro Quantidade de noites em finais de semana reservadas number of week nights Inteiro Quantidade de noites em dias de semana reservadas type of meal String Plano de alimenta\u00e7\u00e3o escolhido pelo cliente car parking space Inteiro Vari\u00e1vel bin\u00e1ria que indica se um estacionamento de carro foi pedido ou incluso na reserva room type String Tipo de quarto reservado lead time Inteiro N\u00famero de dias entre a data da reserva e a data de chegada do cliente market segment type String Tipo de segmento do mercado associado \u00e0 reserva repeated Inteiro Vari\u00e1vel bin\u00e1ria que indica se a reserva \u00e9, ou n\u00e3o, repetida P-C Inteiro N\u00famero de reservas anteriores que foram canceladas pelo cliente antes do agendamento atual P-not-C Inteiro N\u00famero de reservas anteriores que n\u00e3o foram canceladas pelo cliente antes do agendamento atual average price Float Pre\u00e7o m\u00e9dio associado \u00e0 reserva special requests Inteiro N\u00famero de pedidos especiais feitos pelo convidado(a) date of reservation String Data da reserva booking status String Status da reserva (cancelada ou n\u00e3o cancelada)"},{"location":"knn/main/#visualizacoes-das-variaveis","title":"Visualiza\u00e7\u00f5es das vari\u00e1veis","text":"<p>Em seguida, \u00e9 essencial realizar gr\u00e1ficos para visualizar como cada uma das vari\u00e1veis se comportam, com o objetivo de entender melhor a base da dados.</p> <p>Est\u00e1 se\u00e7\u00e3o ser\u00e1 divida para cada tipo de vari\u00e1vel, entre vari\u00e1veis quantitativas discretas, quantitativas cont\u00ednuas, qualitativas categ\u00f3ricas, bin\u00e1rias e, por fim, a vari\u00e1vel alvo.</p>"},{"location":"knn/main/#variaveis-quantitativas-discretas","title":"Vari\u00e1veis Quantitativas Discretas","text":"number of adultsnumber of childrennumber of weekend nightsnumber of week nightslead timeP-CP-not-Cspecial requests Gr\u00e1ficoC\u00f3digo 2025-10-29T22:00:18.766742 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/knn/booking.csv\")\n\nplt.figure(figsize=(10, 6))\ndf[\"number of adults\"].value_counts().sort_index().plot(kind=\"bar\", color=\"skyblue\", edgecolor=\"black\")\nplt.title(\"Distribui\u00e7\u00e3o do N\u00famero de Adultos por Reserva - Barras\")\nplt.xlabel(\"N\u00famero de Adultos\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.xticks(rotation=0)\nplt.grid(axis=\"y\", alpha=0.3)\n\nax = plt.gca()\nfor p in ax.patches:\n    ax.annotate(f\"{int(p.get_height())}\", \n                (p.get_x() + p.get_width()/2., p.get_height()),\n                ha=\"center\", va=\"bottom\")\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> Gr\u00e1ficoC\u00f3digo 2025-10-29T22:00:18.975785 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/knn/booking.csv\")\n\nplt.figure(figsize=(10, 6))\ndf[\"number of children\"].value_counts().sort_index().plot(kind=\"bar\", color=\"lightcoral\", edgecolor=\"black\")\nplt.title(\"Distribui\u00e7\u00e3o do N\u00famero de Crian\u00e7as por Reserva - Barras\")\nplt.xlabel(\"N\u00famero de Crian\u00e7as\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.xticks(rotation=0)\nplt.grid(axis=\"y\", alpha=0.3)\n\nax = plt.gca()\nfor p in ax.patches:\n    ax.annotate(f\"{int(p.get_height())}\", \n                (p.get_x() + p.get_width()/2., p.get_height()),\n                ha=\"center\", va=\"bottom\")\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> Gr\u00e1ficoC\u00f3digo 2025-10-29T22:00:19.201798 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/knn/booking.csv\")\n\nplt.figure(figsize=(10, 6))\ndf[\"number of weekend nights\"].value_counts().sort_index().plot(kind=\"bar\", color=\"lightgreen\", edgecolor=\"black\")\nplt.title(\"Distribui\u00e7\u00e3o de Noites de Fim de Semana por Reserva - Barras\")\nplt.xlabel(\"N\u00famero de Noites de Fim de Semana\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.xticks(rotation=0)\nplt.grid(axis=\"y\", alpha=0.3)\n\nax = plt.gca()\nfor p in ax.patches:\n    ax.annotate(f\"{int(p.get_height())}\", \n                (p.get_x() + p.get_width()/2., p.get_height()),\n                ha=\"center\", va=\"bottom\")\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> Gr\u00e1ficoC\u00f3digo 2025-10-29T22:00:19.422423 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/knn/booking.csv\")\n\nplt.figure(figsize=(10, 6))\ndf[\"number of week nights\"].value_counts().sort_index().plot(kind=\"bar\", color=\"gold\", edgecolor=\"black\")\nplt.title(\"Distribui\u00e7\u00e3o de Noites de Dias de Semana por Reserva - Barras\")\nplt.xlabel(\"N\u00famero de Noites de Dias de Semana\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.xticks(rotation=0)\nplt.grid(axis=\"y\", alpha=0.3)\n\nax = plt.gca()\nfor p in ax.patches:\n    ax.annotate(f\"{int(p.get_height())}\", \n                (p.get_x() + p.get_width()/2., p.get_height()),\n                ha=\"center\", va=\"bottom\")\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> Gr\u00e1ficoC\u00f3digo 2025-10-29T22:00:19.697175 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/knn/booking.csv\")\n\nplt.figure(figsize=(10, 6))\nlead_time_bins = pd.cut(df[\"lead time\"], bins=8)\nlead_time_bins.value_counts().sort_index().plot(kind=\"bar\", color=\"coral\", edgecolor=\"black\")\nplt.title(\"Distribui\u00e7\u00e3o do Lead Time (dias entre reserva e chegada) - Barras\")\nplt.xlabel(\"Intervalo de Dias\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.xticks(rotation=45)\nplt.grid(axis=\"y\", alpha=0.3)\nplt.tight_layout()\n\nax = plt.gca()\nfor p in ax.patches:\n    ax.annotate(f\"{int(p.get_height())}\", \n                (p.get_x() + p.get_width()/2., p.get_height()),\n                ha=\"center\", va=\"bottom\")\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> Gr\u00e1ficoC\u00f3digo 2025-10-29T22:00:19.907206 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/knn/booking.csv\")\n\nplt.figure(figsize=(10, 6))\ndf[\"P-C\"].value_counts().sort_index().plot(kind=\"bar\", color=\"orange\", edgecolor=\"black\")\nplt.title(\"Distribui\u00e7\u00e3o de Reservas Anteriormente Canceladas (P-C) - Barras\")\nplt.xlabel(\"N\u00famero de Reservas Anteriormente Canceladas\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.xticks(rotation=0)\nplt.grid(axis=\"y\", alpha=0.3)\n\nax = plt.gca()\nfor p in ax.patches:\n    ax.annotate(f\"{int(p.get_height())}\", \n                (p.get_x() + p.get_width()/2., p.get_height()),\n                ha=\"center\", va=\"bottom\")\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> Gr\u00e1ficoC\u00f3digo 2025-10-29T22:00:20.105752 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/knn/booking.csv\")\n\nplt.figure(figsize=(10, 6))\nplt.hist(df[\"P-not-C\"], bins=15, edgecolor=\"black\", alpha=0.7, color=\"blue\")\nplt.title(\"Distribui\u00e7\u00e3o de Reservas Anteriormente N\u00e3o Canceladas (P-not-C) - Histograma\")\nplt.xlabel(\"N\u00famero de Reservas Anteriormente N\u00e3o Canceladas\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.grid(axis=\"y\", alpha=0.3)\n\nax = plt.gca()\nfor p in ax.patches:\n    ax.annotate(f\"{int(p.get_height())}\", \n                (p.get_x() + p.get_width()/2., p.get_height()),\n                ha=\"center\", va=\"bottom\")\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> Gr\u00e1ficoC\u00f3digo 2025-10-29T22:00:20.316011 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/knn/booking.csv\")\n\nplt.figure(figsize=(10, 6))\ndf[\"special requests\"].value_counts().sort_index().plot(kind=\"bar\", color=\"purple\", edgecolor=\"black\")\nplt.title(\"Distribui\u00e7\u00e3o de Pedidos Especiais por Reserva - Barras\")\nplt.xlabel(\"N\u00famero de Pedidos Especiais\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.xticks(rotation=0)\nplt.grid(axis=\"y\", alpha=0.3)\n\nax = plt.gca()\nfor p in ax.patches:\n    ax.annotate(f\"{int(p.get_height())}\", \n                (p.get_x() + p.get_width()/2., p.get_height()),\n                ha=\"center\", va=\"bottom\")\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre>"},{"location":"knn/main/#variavel-quantitativa-continua-average-price","title":"Vari\u00e1vel Quantitativa Cont\u00ednua <code>average price</code>","text":"Gr\u00e1ficoC\u00f3digo 2025-10-29T22:00:20.512813 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/knn/booking.csv\")\n\nplt.figure(figsize=(10, 6))\nplt.hist(df[\"average price\"], bins=30, edgecolor=\"black\", alpha=0.7, color=\"red\")\nplt.title(\"Distribui\u00e7\u00e3o do Pre\u00e7o M\u00e9dio das Reservas - Histograma\")\nplt.xlabel(\"Pre\u00e7o M\u00e9dio\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.grid(axis=\"y\", alpha=0.3)\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre>"},{"location":"knn/main/#variaveis-categoricas","title":"Vari\u00e1veis Categ\u00f3ricas","text":"type of mealroom typemarket segment type Gr\u00e1ficoC\u00f3digo 2025-10-29T22:00:20.711761 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/knn/booking.csv\")\n\nplt.figure(figsize=(10, 8))\ndf[\"type of meal\"].value_counts().plot(kind=\"bar\", color=\"lightseagreen\", edgecolor=\"black\")\nplt.title(\"Distribui\u00e7\u00e3o dos Tipos de Refei\u00e7\u00e3o - Barras\")\nplt.xlabel(\"Tipo de Refei\u00e7\u00e3o\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.xticks(rotation=45)\nplt.grid(axis=\"y\", alpha=0.3)\n\nax = plt.gca()\nfor p in ax.patches:\n    ax.annotate(f\"{int(p.get_height())}\", \n                (p.get_x() + p.get_width()/2., p.get_height()),\n                ha=\"center\", va=\"bottom\")\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> Gr\u00e1ficoC\u00f3digo 2025-10-29T22:00:20.907789 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/knn/booking.csv\")\n\nplt.figure(figsize=(10, 8))\ndf[\"room type\"].value_counts().plot(kind=\"bar\", color=\"mediumpurple\", edgecolor=\"black\")\nplt.title(\"Distribui\u00e7\u00e3o dos Tipos de Quarto - Barras\")\nplt.xlabel(\"Tipo de Quarto\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.xticks(rotation=45)\nplt.grid(axis=\"y\", alpha=0.3)\n\nax = plt.gca()\nfor p in ax.patches:\n    ax.annotate(f\"{int(p.get_height())}\", \n                (p.get_x() + p.get_width()/2., p.get_height()),\n                ha=\"center\", va=\"bottom\")\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> Gr\u00e1ficoC\u00f3digo 2025-10-29T22:00:21.098968 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/knn/booking.csv\")\n\nplt.figure(figsize=(10, 8))\ndf[\"market segment type\"].value_counts().plot(kind=\"bar\", color=\"salmon\", edgecolor=\"black\")\nplt.title(\"Distribui\u00e7\u00e3o dos Segmentos de Mercado - Barras\")\nplt.xlabel(\"Segmento de Mercado\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.xticks(rotation=45)\nplt.grid(axis=\"y\", alpha=0.3)\n\nax = plt.gca()\nfor p in ax.patches:\n    ax.annotate(f\"{int(p.get_height())}\", \n                (p.get_x() + p.get_width()/2., p.get_height()),\n                ha=\"center\", va=\"bottom\")\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre>"},{"location":"knn/main/#variaveis-binarias","title":"Vari\u00e1veis Bin\u00e1rias","text":"car parking spacerepeated Gr\u00e1ficoC\u00f3digo 2025-10-29T22:00:21.278994 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/knn/booking.csv\")\n\nlabels = [\"Sem estacionamento\", \"Com estacionamento\"]  \nvalues = df[\"car parking space\"].value_counts()\n\nplt.figure(figsize=(10, 8))\nplt.pie(values, labels=labels, autopct=lambda p: f\"{int(p * sum(values) / 100)}\", colors=[\"lightblue\", \"lightcoral\"])\nplt.title(\"Distribui\u00e7\u00e3o do Status das Reservas - Pizza\")\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> Gr\u00e1ficoC\u00f3digo 2025-10-29T22:00:21.434864 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/knn/booking.csv\")\n\nlabels = [\"N\u00e3o Repetido\", \"Repetido\"]  \nvalues = df[\"repeated\"].value_counts()\n\nplt.figure(figsize=(10, 8))\nplt.pie(values, labels=labels, autopct=lambda p: f\"{int(p * sum(values) / 100)}\", colors=[\"lightgreen\", \"lightyellow\"])\nplt.title(\"Distribui\u00e7\u00e3o do Status das Reservas - Pizza\")\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre>"},{"location":"knn/main/#variavel-alvo-booking-status","title":"Vari\u00e1vel Alvo <code>booking status</code>","text":"Gr\u00e1ficoC\u00f3digo 2025-10-29T22:00:21.590806 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/knn/booking.csv\")\n\nlabels = [\"Cancelado\", \"N\u00e3o Cancelado\"]  \nvalues = df[\"booking status\"].value_counts()\n\nplt.figure(figsize=(10, 8))\nplt.pie(values, labels=labels, autopct=lambda p: f\"{int(p * sum(values) / 100)}\", colors=[\"lightcoral\", \"lightgreen\"])\nplt.title(\"Distribui\u00e7\u00e3o do Status das Reservas - Pizza\")\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> <p>Atrav\u00e9s das an\u00e1lises, foi poss\u00edvel alcan\u00e7ar uma compreens\u00e3o mais aprofundada do funcionamento de cada uma das vari\u00e1veis no dataset, al\u00e9m de haver insights valiosos nesses gr\u00e1ficos. Esses dados ser\u00e3o essenciais para a escolha das vari\u00e1veis que ser\u00e3o utilizadas no modelo.</p>"},{"location":"knn/main/#etapa-2-pre-processamento-e-divisao-de-dados","title":"Etapa 2 - Pr\u00e9-processamento e Divis\u00e3o de Dados","text":"<p>Neste projeto, ap\u00f3s um estudo do pr\u00e9-processamento e divis\u00e3o de dados, foram considerados dois modelos distintos de pr\u00e9-processamento. O primeiro modelo faz, primeiro, o pr\u00e9-processamento, utilizando todo o dataset para treinar o modelo de predi\u00e7\u00e3o. O segundo modelo cria o pr\u00e9-processamento apenas com os dados de treinamento, evitando que o modelo tenha acesso indireto aos dados de teste, evitando data leakage.</p> <p>O primeiro modelo utiliza os dados de teste para realizar a padroniza\u00e7\u00e3o e substitui\u00e7\u00e3o de valores nulos no dataset inteiro, fazendo com que, indiretamente, o modelo tenha acesso aos dados de teste. Esse problema pode afetar a acur\u00e1cia do modelo com enviesamento, fazendo com que sua efic\u00e1cia real seja diferente da testada. O segundo modelo trata os dados de teste como dados que nunca foram acessados pelo modelo. O pr\u00e9-processamento, depois de feito a partir dos dados de treino, ser\u00e1 aplicado aos dados de teste, inserindo-os no mesmo dom\u00ednio do modelo para que possam ser realizadas predi\u00e7\u00f5es. A hip\u00f3tese principal \u00e9 de que a acur\u00e1cia do segundo modelo ser\u00e1 um pouco menor, mas o modelo ter\u00e1 menos vi\u00e9s.</p> <p>Abaixo, est\u00e3o o diagramas de sequ\u00eancia representando cada modelo:</p>"},{"location":"knn/main/#modelo-1-pre-processamento-divisao-dos-dados","title":"Modelo 1 - Pr\u00e9-processamento -&gt; Divis\u00e3o dos Dados","text":"<pre><code>flowchart TD\n    A[Explora\u00e7\u00e3o de Dados] --&gt; B[Pr\u00e9-processamento]\n    B --&gt; C{Divis\u00e3o dos Dados}\n    C --&gt;|80% dos dados| D[Treino] --&gt; F[Treinamento do modelo]\n    C --&gt;|20% dos dados| E[Teste] --&gt; G[Avalia\u00e7\u00e3o do modelo]\n    F --&gt; G</code></pre>"},{"location":"knn/main/#modelo-2-divisao-dos-dados-pre-processamento","title":"Modelo 2 Divis\u00e3o dos Dados -&gt; Pr\u00e9-processamento","text":"<pre><code>flowchart TD\n    A[Explora\u00e7\u00e3o de Dados] --&gt; B{Divis\u00e3o dos Dados}\n    B --&gt;|Teste&lt;br&gt;20% dos dados| PTest[Pr\u00e9-processamento]\n    B --&gt;|Treino&lt;br&gt;80% dos dados| PTrain[Pr\u00e9-processamento]\n    PTrain --&gt; Train[Treinamento]\n    Train --&gt; G\n    PTest --&gt; G[Avalia\u00e7\u00e3o do modelo]</code></pre> <p>Nos dois modelos, o pr\u00e9-processamento \u00e9 o mesmo. O que muda \u00e9 o conjunto de dados em que ele \u00e9 aplicado, sendo aplicado em todo o dataset no modelo 1 e apenas no conjunto de treino no modelo 2.</p>"},{"location":"knn/main/#1-passo-identificacao-e-tratamento-de-valores-nulos","title":"1\u00b0 Passo: Identifica\u00e7\u00e3o e tratamento de valores nulos","text":"<p>O primeiro passo para o pr\u00e9-processamento \u00e9 identificar e tratar valores nulos na base.</p> <pre><code>print(df.isna().sum())\n</code></pre> <p>Executando a linha de c\u00f3digo acima para o dataframe contendo os dados da base, foi poss\u00edvel identificar que n\u00e3o h\u00e1 valores nulos na base.</p>"},{"location":"knn/main/#2-passo-remocao-de-colunas-desimportantes","title":"2\u00b0 Passo: Remo\u00e7\u00e3o de colunas desimportantes","text":"<p>Em seguida, colunas que n\u00e3o s\u00e3o importantes para a predi\u00e7\u00e3o ser\u00e3o removidas do dataframe. Essas colunas s\u00e3o <code>Booking_ID</code> e <code>date of reservation</code>. A forma que essa exclus\u00e3o foi feita est\u00e1 representada abaixo:</p> <pre><code>df = df.drop(columns=[\"Booking_ID\", \"date of reservation\"])\n</code></pre>"},{"location":"knn/main/#3-passo-codificacao-de-variaveis-categoricas","title":"3\u00b0 Passo: Codifica\u00e7\u00e3o de vari\u00e1veis categ\u00f3ricas","text":"<p>O terceiro passo se consiste na codifica\u00e7\u00e3o das vari\u00e1veis categ\u00f3ricas. Essas s\u00e3o: <code>type of meal</code>, <code>room type</code> e <code>market segment type</code>. Considerando a forma como a t\u00e9cnica do KNN funciona, calculando a dist\u00e2ncia euclidiana entre pontos para predizer, a t\u00e9cnica de label encoding seria ruim, pois os valores num\u00e9ricos arbitr\u00e1rios poderiam criar dist\u00e2ncias falsas entre as categorias. Por isso, utilizaremos a t\u00e9cnica de One-Hot Encoding para codificar essas vari\u00e1veis, utilizando o OneHotEncoder() do <code>scikit-learn</code>.</p> <p>Modelo 1:</p> <pre><code>from sklearn.preprocessing import OneHotEncoder\n\nencoder = OneHotEncoder()\ncategorical_cols = [\"type of meal\", \"room type\", \"market segment type\"]\n\nX = df.drop(\"booking status\", axis=1)\n\nX_encoded = encoder.fit_transform(X[categorical_cols])\nencoded_df = pd.DataFrame(X_encoded.toarray(), columns=encoder.get_feature_names_out(categorical_cols), index=X.index)\n\nX = pd.concat([X.drop(columns=categorical_cols), encoded_df], axis=1)\n</code></pre> <p>Modelo 2:</p> <pre><code>from sklearn.preprocessing import OneHotEncoder\n\ncategorical_cols = [\"type of meal\", \"room type\", \"market segment type\"]\n\nX = df.drop(\"booking status\", axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nencoder = OneHotEncoder(drop=\"first\", sparse_output=False)\nencoder.fit(X_train[categorical_cols])\n\nX_train_encoded = encoder.transform(X_train[categorical_cols])\nX_test_encoded = encoder.transform(X_test[categorical_cols])\n</code></pre>"},{"location":"knn/main/#4-passo-padronizacao-das-features-numericas","title":"4\u00b0 Passo: Padroniza\u00e7\u00e3o das features num\u00e9ricas","text":"<p>Em seguida, \u00e9 necess\u00e1ria a padroniza\u00e7\u00e3o das features num\u00e9ricas na base. Ao inv\u00e9s da normaliza\u00e7\u00e3o, ser\u00e1 utilizada a t\u00e9cnica de padroniza\u00e7\u00e3o devido aos outliers nas features num\u00e9ricas, principalmente as vari\u00e1veis <code>lead time</code> e <code>average price</code>, que desbalanceariam o c\u00e1lculo de dist\u00e2ncias se apenas normalizadas. Para a padroniza\u00e7\u00e3o, utilizaremos o StandardScaler() do <code>scikit-learn</code>.</p> <p>Modelo 1:</p> <pre><code>from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nnumeric_cols = [\"number of adults\", \"number of children\", \"number of weekend nights\", \n                \"number of week nights\", \"lead time\", \"P-C\", \"P-not-C\", \n                \"average price\", \"special requests\"]\n\nX = df.drop(\"booking status\", axis=1)\n\nfor col in numeric_cols:\n    X[col] = scaler.fit_transform(X[[col]])\n</code></pre> <p>Modelo 2:</p> <pre><code>from sklearn.preprocessing import StandardScaler\n\nnumeric_cols = [\"number of adults\", \"number of children\", \"number of weekend nights\", \n                \"number of week nights\", \"lead time\", \"P-C\", \"P-not-C\", \n                \"average price\", \"special requests\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n</code></pre>"},{"location":"knn/main/#5-passo-codificacao-da-variavel-alvo","title":"5\u00b0 Passo: Codifica\u00e7\u00e3o da vari\u00e1vel alvo","text":"<p>Por fim, vamos codificar a vari\u00e1vel alvo <code>booking status</code> utilizando a t\u00e9cnica de label encoding. Ou seja, ap\u00f3s esse passo, \"Not_Canceled\" vai assumir o valor 1 e \"Canceled\" o valor 0. Aqui, essa t\u00e9cnica pode ser utilizada, pois essa \u00e9 a vari\u00e1vel alvo, e n\u00e3o ser\u00e1 utilizada no c\u00e1lculo das dist\u00e2ncias. Para codificar, utilizaremos o LabelEncoder() do <code>scikit-learn</code>.</p> <p>Modelo 1 e 2:</p> <pre><code>from sklearn.preprocessing import LabelEncoder\n\nl_encoder = LabelEncoder()\ny = l_encoder.fit_transform(df[\"booking status\"])\n</code></pre>"},{"location":"knn/main/#divisao-dos-dados","title":"Divis\u00e3o dos dados","text":"<p>Como explicado anteriormente, essa etapa ser\u00e1 realizada em momentos distintos dependendo do modelo utilizado. No primeiro modelo, esta etapa vem depois de todo o pr\u00e9-processamento. No segundo modelo, esta etapa vem antes do pr\u00e9-processamento.</p> <ul> <li> <p>Conjunto de Treino: Utilizado para ensinar o modelo a reconhecer padr\u00f5es</p> </li> <li> <p>Conjunto de Teste: Utilizado para avaliar o desempenho do modelo com dados ainda n\u00e3o vistos</p> </li> </ul> <p>Para realizar a divis\u00e3o, foi utilizada a fun\u00e7\u00e3o train_test_split() do <code>scikit-learn</code>. Os par\u00e2metros utilizados s\u00e3o:</p> <ul> <li> <p>test_size=0.2: Define que 20% dos dados ser\u00e3o utilizados para teste, enquanto o restante ser\u00e1 usado para treino.</p> </li> <li> <p>random_state=42: Par\u00e2metro que controla o gerador de n\u00famero aleat\u00f3rios utilizado para sortear os dados antes de separ\u00e1-los. Garante reprodutibilidade.</p> </li> <li> <p>stratify=y: Esse atributo definido como y \u00e9 essencial devido \u00e0 natureza da coluna <code>booking status</code>. Com essa defini\u00e7\u00e3o, ser\u00e1 mantida a mesma propor\u00e7\u00e3o das categorias em ambos os conjuntos, reduzindo o vi\u00e9s.</p> </li> </ul> Sa\u00eddaC\u00f3digo <p>Treino: 29028 amostras</p> <p>Teste: 7257 amostras</p> <p>Propor\u00e7\u00e3o: 80.0% treino, 20.0% teste</p> <p>Distribui\u00e7\u00e3o das classes - </p> <p>Treino:</p> booking status count Not_Canceled 19517 Canceled 9511 <p>Teste:</p> booking status count Not_Canceled 4879 Canceled 2378 <pre><code>import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndf = pd.read_csv(\"docs/knn/booking.csv\")\n\ndf = df.drop(columns=[\"Booking_ID\", \"date of reservation\"])\n\nX = df.drop(\"booking status\", axis=1)\ny = df[\"booking status\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\nprint(f\"Treino: {X_train.shape[0]} amostras\\n\")\nprint(f\"Teste: {X_test.shape[0]} amostras\\n\")\nprint(f\"Propor\u00e7\u00e3o: {X_train.shape[0]/X.shape[0]*100:.1f}% treino, {X_test.shape[0]/X.shape[0]*100:.1f}% teste\\n\")\n\nprint(\"Distribui\u00e7\u00e3o das classes - \\n\")\nprint(\"Treino:\\n\")\nprint(y_train.value_counts().to_markdown(), \"\\n\")\nprint(\"Teste:\\n\")\nprint(y_test.value_counts().to_markdown(), \"\\n\")\n</code></pre> <p>Esta divis\u00e3o adequada \u00e9 de extrema import\u00e2ncia, pois ajuda a evitar overfitting.</p>"},{"location":"knn/main/#etapa-3-treinamento-dos-modelos","title":"Etapa 3 - Treinamento dos Modelos","text":"<p>Agora, ser\u00e1 realizado o treinamento dos modelos. O objetivo dessa etapa \u00e9 ensinar o algoritmo a reconhecer padr\u00f5es nos dados que s\u00e3o fornecidos, e determinar se uma reserva ser\u00e1, ou n\u00e3o, cancelada de acordo com os dados das outras vari\u00e1veis na base.</p> <p>Para visualizar a efic\u00e1cia dos modelos, foi aplicado um PCA (Principal Component Analysis) para definir as melhores vari\u00e1veis a serem visualizadas. Al\u00e9m disso, foram feitas matrizes de confus\u00e3o dos dois modelos.</p>"},{"location":"knn/main/#resultado-dos-treinamentos","title":"Resultado dos treinamentos","text":"<p>Modelo 1:</p> KNN - Modelo 1C\u00f3digo <p></p> Acur\u00e1cia: 0.8541 <p></p> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.decomposition import PCA\n\nencoder = OneHotEncoder()\nscaler = StandardScaler()\nl_encoder = LabelEncoder()\n\ndf = pd.read_csv(\"docs/knn/booking.csv\")\n\ndf = df.drop(columns=[\"Booking_ID\", \"date of reservation\"])\n\nnumeric_cols = [\"number of adults\", \"number of children\", \"number of weekend nights\", \n                \"number of week nights\", \"lead time\", \"P-C\", \"P-not-C\", \n                \"average price\", \"special requests\"]\n\ncategorical_cols = [\"type of meal\", \"room type\", \"market segment type\"]\n\nX = df.drop(\"booking status\", axis=1)\ny = l_encoder.fit_transform(df[\"booking status\"])\n\nfor col in numeric_cols:\n    X[col] = scaler.fit_transform(X[[col]])\n\nX_encoded = encoder.fit_transform(X[categorical_cols])\n\nencoded_df = pd.DataFrame(X_encoded.toarray(), columns=encoder.get_feature_names_out(categorical_cols), index=X.index)\n\nX = pd.concat([X.drop(columns=categorical_cols), encoded_df], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, y_train)\npredictions = knn.predict(X_test)\nprint(f\"Accuracy: {accuracy_score(y_test, predictions):.4f}&lt;br&gt;\")\n\n# Visualiza\u00e7\u00e3o do KNN \n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\nprint(\"Variance explained by each component:\", pca.explained_variance_ratio_)\n\nX_train_pca, X_test_pca, _, _ = train_test_split(X_pca, y, test_size=0.2, random_state=42)\nknn_pca = KNeighborsClassifier(n_neighbors=3)\nknn_pca.fit(X_train_pca, y_train)\n\nh = .05\nx_min, x_max = X_pca[:, 0].min() - 1, X_pca[:, 0].max() + 1\ny_min, y_max = X_pca[:, 1].min() - 1, X_pca[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\nnp.arange(y_min, y_max, h))\n\nZ = knn_pca.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.figure(figsize=(10, 7))\nplt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)\nsns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y, palette=\"coolwarm\", edgecolor=\"k\", s=60)\nplt.title(\"KNN com PCA do Modelo 1)\")\nplt.xlabel(\"Componente Principal 1\")\nplt.ylabel(\"Componente Principal 2\")\nplt.legend(title=\"Booking status\")\n\nplt.savefig(\"docs/images/knn_modelo1.svg\", format=\"svg\", transparent=True)\nplt.close()\n</code></pre> <p>Modelo 2:</p> KNN - Modelo 2C\u00f3digo <p></p> Acur\u00e1cia: 0.8521 <p></p> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.decomposition import PCA\n\nencoder = OneHotEncoder()\nscaler = StandardScaler()\nl_encoder = LabelEncoder()\n\ndf = pd.read_csv(\"docs/knn/booking.csv\")\n\ndf = df.drop(columns=[\"Booking_ID\", \"date of reservation\"])\n\nnumeric_cols = [\"number of adults\", \"number of children\", \"number of weekend nights\", \n                \"number of week nights\", \"lead time\", \"P-C\", \"P-not-C\", \n                \"average price\", \"special requests\"]\n\ncategorical_cols = [\"type of meal\", \"room type\", \"market segment type\"]\n\nX = df.drop(\"booking status\", axis=1)\ny = l_encoder.fit_transform(df[\"booking status\"])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nscaler.fit(X_train[numeric_cols])\nencoder.fit(X_train[categorical_cols])\n\nX_train_scaled = scaler.transform(X_train[numeric_cols])\nX_test_scaled = scaler.transform(X_test[numeric_cols])\n\nX_train_encoded = encoder.transform(X_train[categorical_cols]).toarray()\nX_test_encoded = encoder.transform(X_test[categorical_cols]).toarray()\n\nX_train_final = np.concatenate([X_train_scaled, X_train_encoded], axis=1)\nX_test_final = np.concatenate([X_test_scaled, X_test_encoded], axis=1)\n\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train_final, y_train)\npredictions = knn.predict(X_test_final)\nprint(f\"Accuracy: {accuracy_score(y_test, predictions):.4f} &lt;br&gt;\")\n\n# Visualiza\u00e7\u00e3o do KNN \n\npca = PCA(n_components=2)\nX_train_pca = pca.fit_transform(X_train_final)\nX_test_pca = pca.transform(X_test_final)\nprint(\"Variance explained by each component:\", pca.explained_variance_ratio_)\n\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train_pca, y_train)\npredictions = knn.predict(X_test_pca)\n\nplt.figure(figsize=(12, 8))\n\nh = 0.05\nx_min, x_max = X_train_pca[:, 0].min() - 1, X_train_pca[:, 0].max() + 1\ny_min, y_max = X_train_pca[:, 1].min() - 1, X_train_pca[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\nZ = knn.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu, alpha=0.3)\n\nsns.scatterplot(x=X_train_pca[:, 0], y=X_train_pca[:, 1], hue=y_train,\n                palette=\"deep\", edgecolor=\"k\", s=80)\nsns.scatterplot(x=X_test_pca[:, 0], y=X_test_pca[:, 1], hue=y_test,\n                palette=\"deep\", edgecolor=\"k\", marker=\"X\", s=120)\n\nplt.xlabel(\"Componente Principal 1\")\nplt.ylabel(\"Componente Principal 2\")\nplt.title(\"KNN com PCA do Modelo 2\")\nplt.legend()\n\nplt.savefig(\"docs/images/knn_modelo2.svg\", format=\"svg\", transparent=True)\nplt.close()\n</code></pre>"},{"location":"knn/main/#etapa-4-avaliacao-dos-modelos","title":"Etapa 4 - Avalia\u00e7\u00e3o dos modelos","text":""},{"location":"knn/main/#matrizes-de-confusao","title":"Matrizes de confus\u00e3o","text":"<p>Primeiramente, vamos observar as matrizes de confus\u00e3o de ambos os modelos. A matriz de confus\u00e3o consegue nos oferecer diversas m\u00e9tricas de qualidade do modelo, como o n\u00famero de previs\u00f5es correta para positivos e negativos, os falso positivos, falso negativos, a precis\u00e3o, especificidade, dentre outras m\u00e9tricas.</p> <p>Modelo 1:</p> Matriz de confus\u00e3o - Modelo 1C\u00f3digo <p></p> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\nfrom sklearn.metrics import confusion_matrix\n\nencoder = OneHotEncoder()\nscaler = StandardScaler()\nl_encoder = LabelEncoder()\n\ndf = pd.read_csv(\"docs/knn/booking.csv\")\n\ndf = df.drop(columns=[\"Booking_ID\", \"date of reservation\"])\n\nnumeric_cols = [\"number of adults\", \"number of children\", \"number of weekend nights\", \n                \"number of week nights\", \"lead time\", \"P-C\", \"P-not-C\", \n                \"average price\", \"special requests\"]\n\ncategorical_cols = [\"type of meal\", \"room type\", \"market segment type\"]\n\nX = df.drop(\"booking status\", axis=1)\ny = l_encoder.fit_transform(df[\"booking status\"])\n\nfor col in numeric_cols:\n    X[col] = scaler.fit_transform(X[[col]])\n\nX_encoded = encoder.fit_transform(X[categorical_cols])\n\nencoded_df = pd.DataFrame(X_encoded.toarray(), columns=encoder.get_feature_names_out(categorical_cols), index=X.index)\n\nX = pd.concat([X.drop(columns=categorical_cols), encoded_df], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, y_train)\npredictions = knn.predict(X_test)\n\ncm = confusion_matrix(y_test, predictions)\n\nplt.figure(figsize=(6,4))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=l_encoder.classes_, yticklabels=l_encoder.classes_)\nplt.xlabel(\"Predito\")\nplt.ylabel(\"Real\")\nplt.title(\"Matriz de Confus\u00e3o - KNN Modelo 1\")\n\nplt.savefig(\"docs/images/cm_knn_modelo1.svg\", format=\"svg\", transparent=True)\nplt.close()\n</code></pre> <p>Modelo 2:</p> Matriz de confus\u00e3o - Modelo 2C\u00f3digo <p></p> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\nfrom sklearn.metrics import confusion_matrix\n\nencoder = OneHotEncoder()\nscaler = StandardScaler()\nl_encoder = LabelEncoder()\n\ndf = pd.read_csv(\"docs/knn/booking.csv\")\n\ndf = df.drop(columns=[\"Booking_ID\", \"date of reservation\"])\n\nnumeric_cols = [\"number of adults\", \"number of children\", \"number of weekend nights\", \n                \"number of week nights\", \"lead time\", \"P-C\", \"P-not-C\", \n                \"average price\", \"special requests\"]\n\ncategorical_cols = [\"type of meal\", \"room type\", \"market segment type\"]\n\nX = df.drop(\"booking status\", axis=1)\ny = l_encoder.fit_transform(df[\"booking status\"])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nscaler.fit(X_train[numeric_cols])\nencoder.fit(X_train[categorical_cols])\n\nX_train_scaled = scaler.transform(X_train[numeric_cols])\nX_test_scaled = scaler.transform(X_test[numeric_cols])\n\nX_train_encoded = encoder.transform(X_train[categorical_cols]).toarray()\nX_test_encoded = encoder.transform(X_test[categorical_cols]).toarray()\n\nX_train_final = np.concatenate([X_train_scaled, X_train_encoded], axis=1)\nX_test_final = np.concatenate([X_test_scaled, X_test_encoded], axis=1)\n\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train_final, y_train)\npredictions = knn.predict(X_test_final)\n\ncm = confusion_matrix(y_test, predictions)\n\nplt.figure(figsize=(6,4))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=l_encoder.classes_, yticklabels=l_encoder.classes_)\nplt.xlabel(\"Predito\")\nplt.ylabel(\"Real\")\nplt.title(\"Matriz de Confus\u00e3o - KNN Modelo 2\")\n\nplt.savefig(\"docs/images/cm_knn_modelo2.svg\", format=\"svg\", transparent=True)\nplt.close()\n</code></pre>"},{"location":"knn/main/#acuracia-dos-modelos","title":"Acur\u00e1cia dos modelos","text":"<p>Os modelos tiveram uma acur\u00e1cia muito pr\u00f3xima, ambas decentes, de 85,41% para o modelo 1 e 85,21% para o modelo 2.</p>"},{"location":"knn/main/#analise-das-visualizacoes","title":"An\u00e1lise das visualiza\u00e7\u00f5es","text":"<p>Modelo 1:</p> <ul> <li> <p>Padr\u00e3o visual: Separa\u00e7\u00e3o mais \"limpa\" entre classes</p> </li> <li> <p>Componentes principais: Distribui\u00e7\u00e3o mais organizada</p> </li> </ul> <p>Modelo 2:</p> <ul> <li> <p>Padr\u00e3o visual: Separa\u00e7\u00e3o mais realista entre classes</p> </li> <li> <p>Componentes principais: Sobreposi\u00e7\u00e3o natural entre clusters</p> </li> </ul>"},{"location":"knn/main/#avaliacao-detalhada-entre-os-modelos","title":"Avalia\u00e7\u00e3o detalhada entre os modelos","text":""},{"location":"knn/main/#modelo-1","title":"Modelo 1","text":"<p>Pontos Fortes:</p> <ul> <li> <p>Acur\u00e1cia ligeiramente superior (86.04% vs 85.80%)</p> </li> <li> <p>Separa\u00e7\u00e3o visual mais clara no espa\u00e7o PCA</p> </li> </ul> <p>Pontos Fracos:</p> <ul> <li> <p>Performance artificialmente inflada</p> </li> <li> <p>Baixa confiabilidade para dados novos</p> </li> <li> <p>N\u00e3o representa cen\u00e1rios do mundo real</p> </li> </ul>"},{"location":"knn/main/#modelo-2","title":"Modelo 2","text":"<p>Pontos Fortes:</p> <ul> <li> <p>Performance realista e confi\u00e1vel</p> </li> <li> <p>Melhor generaliza\u00e7\u00e3o para dados n\u00e3o vistos</p> </li> <li> <p>Aplic\u00e1vel em ambiente de produ\u00e7\u00e3o</p> </li> </ul> <p>Pontos Fracos:</p> <ul> <li> <p>Performance ligeiramente inferior em n\u00fameros absolutos</p> </li> <li> <p>Separa\u00e7\u00e3o menos clara no espa\u00e7o PCA</p> </li> </ul>"},{"location":"knn/main/#etapa-5-relatorio-final","title":"Etapa 5 - Relat\u00f3rio Final","text":""},{"location":"knn/main/#recomendacoes-e-conclusoes","title":"Recomenda\u00e7\u00f5es e Conclus\u00f5es","text":"<p>\u00c9 recomendado o Modelo 2 (sem data leakage) porque:</p> <ul> <li> <p>Fornece estimativas realistas de performance</p> </li> <li> <p>\u00c9 mais robusto para dados novos</p> </li> <li> <p>Evita surpresas desagrad\u00e1veis em produ\u00e7\u00e3o</p> </li> <li> <p>Mant\u00e9m performance muito similar (diferen\u00e7a de apenas 0.2%)</p> </li> </ul>"},{"location":"knn/main/#pontos-importantes-observados","title":"Pontos Importantes Observados","text":"<ul> <li> <p>Data leakage cria uma falsa sensa\u00e7\u00e3o de seguran\u00e7a</p> </li> <li> <p>Diferen\u00e7as pequenas em m\u00e9tricas podem indicar problemas grandes</p> </li> <li> <p>A valida\u00e7\u00e3o rigorosa \u00e9 essencial para modelos confi\u00e1veis</p> </li> <li> <p>Performance visual nem sempre se traduz em performance real</p> </li> </ul>"},{"location":"knn/main/#possiveis-proximos-passos-e-melhorias","title":"Poss\u00edveis pr\u00f3ximos passos e melhorias","text":"<ul> <li> <p>Validar ambos modelos em um conjunto de dados totalmente novo</p> </li> <li> <p>Implementar o Modelo 2 em ambiente controlado</p> </li> <li> <p>Monitorar performance cont\u00ednua em produ\u00e7\u00e3o</p> </li> <li> <p>Considerar t\u00e9cnicas de regulariza\u00e7\u00e3o para melhorar generaliza\u00e7\u00e3o</p> </li> </ul>"},{"location":"knn/main/#conclusao-final","title":"Conclus\u00e3o Final","text":"<p>Embora o Modelo 1 apresente m\u00e9tricas ligeiramente superiores, o Modelo 2 \u00e9 significativamente mais confi\u00e1vel e adequado para implanta\u00e7\u00e3o em ambiente real devido \u00e0 aus\u00eancia de data leakage.</p>"},{"location":"metrics/main/","title":"Projeto 4 - M\u00e9tricas e Avalia\u00e7\u00e3o do KNN e K-Means","text":""},{"location":"metrics/main/#metricas-de-qualidade-e-avaliacao-dos-modelos-knn-e-k-means","title":"M\u00e9tricas de Qualidade e Avalia\u00e7\u00e3o dos modelos KNN e K-Means","text":"<p>Aqui, vou documentar mais especificamente e com mais informa\u00e7\u00f5es as m\u00e9tricas e avalia\u00e7\u00e3o desses dois modelos.</p>"},{"location":"metrics/main/#modelo-knn","title":"Modelo KNN","text":"<p>Primeiramente, vamos come\u00e7ar com o modelo KNN. O dataset utilizado possui informa\u00e7\u00f5es sobre reservas em um hotel, e foi utilizado para prever se um agendamento ser\u00e1, ou n\u00e3o, cancelado. No KNN, foi feito um estudo utilizando dois modelos distintos, j\u00e1 explicados no projeto. Portanto, todas as m\u00e9tricas de qualidade ser\u00e3o aplicadas separadamente para ambos os modelos.</p>"},{"location":"metrics/main/#resultados-do-modelo-1","title":"Resultados do Modelo 1","text":"<p>O Modelo 1 pode ser visualizado da seguinte forma:</p> <pre><code>flowchart TD\n    A[Explora\u00e7\u00e3o de Dados] --&gt; B[Pr\u00e9-processamento]\n    B --&gt; C{Divis\u00e3o dos Dados}\n    C --&gt;|80% dos dados| D[Treino] --&gt; F[Treinamento do modelo]\n    C --&gt;|20% dos dados| E[Teste] --&gt; G[Avalia\u00e7\u00e3o do modelo]\n    F --&gt; G</code></pre> <p>Seu resultado pode ser visualizado abaixo:</p> Acur\u00e1cia: 0.8541"},{"location":"metrics/main/#metricas-para-o-modelo-1","title":"M\u00e9tricas para o Modelo 1","text":"<p>Agora, vamos descrever as m\u00e9tricas para o modelo, e explic\u00e1-las.</p>"},{"location":"metrics/main/#matriz-de-confusao","title":"Matriz de Confus\u00e3o","text":"<p>A matriz de confus\u00e3o consegue nos oferecer diversos valores que ser\u00e3o utilizados para calcular as m\u00e9tricas de qualidade.</p>"},{"location":"metrics/main/#metricas-de-qualidade","title":"M\u00e9tricas de Qualidade","text":"M\u00e9tricas - Modelo 1C\u00f3digo <p>Matriz de confus\u00e3o detalhada: Verdadeiros Negativos (TN): 1802 Falsos Positivos (FP): 600 Falsos Negativos (FN): 459 Verdadeiros Positivos (TP): 4396 M\u00e9tricas: Acur\u00e1cia: 0.8541 Sensibilidade (Recall): 0.9055 Especificidade: 0.7502 Precis\u00e3o: 0.8799 Valor Preditivo Negativo: 0.7970 Taxa de Falsos Positivos: 0.2498 Taxa de Falsos Negativos: 0.0945</p> <pre><code>import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n\nencoder = OneHotEncoder()\nscaler = StandardScaler()\nl_encoder = LabelEncoder()\n\ndf = pd.read_csv(\"docs/knn/booking.csv\")\n\ndf = df.drop(columns=[\"Booking_ID\", \"date of reservation\"])\n\nnumeric_cols = [\"number of adults\", \"number of children\", \"number of weekend nights\", \n                \"number of week nights\", \"lead time\", \"P-C\", \"P-not-C\", \n                \"average price\", \"special requests\"]\n\ncategorical_cols = [\"type of meal\", \"room type\", \"market segment type\"]\n\nX = df.drop(\"booking status\", axis=1)\ny = l_encoder.fit_transform(df[\"booking status\"])\n\nfor col in numeric_cols:\n    X[col] = scaler.fit_transform(X[[col]])\n\nX_encoded = encoder.fit_transform(X[categorical_cols])\n\nencoded_df = pd.DataFrame(X_encoded.toarray(), columns=encoder.get_feature_names_out(categorical_cols), index=X.index)\n\nX = pd.concat([X.drop(columns=categorical_cols), encoded_df], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, y_train)\npredictions = knn.predict(X_test)\n\ncm = confusion_matrix(y_test, predictions)\ntn, fp, fn, tp = cm.ravel()\n\nprint(\"&lt;b&gt;Matriz de confus\u00e3o detalhada:&lt;/b&gt;&lt;br&gt;\")\nprint(f\"Verdadeiros Negativos (TN): {tn}&lt;br&gt;\")\nprint(f\"Falsos Positivos (FP): {fp}&lt;br&gt;\")\nprint(f\"Falsos Negativos (FN): {fn}&lt;br&gt;\")\nprint(f\"Verdadeiros Positivos (TP): {tp}&lt;br&gt;\")\n\nacuracia = (tn + tp) / (tn + fp + fn + tp)\nsensibilidade = tp / (tp + fn) if (tp + fn) &gt; 0 else 0\nespecificidade = tn / (tn + fp) if (tn + fp) &gt; 0 else 0\nprecisao = tp / (tp + fp) if (tp + fp) &gt; 0 else 0\nvalor_preditivo_negativo = tn / (tn + fn) if (tn + fn) &gt; 0 else 0\nfalsos_positivos_rate = fp / (fp + tn) if (fp + tn) &gt; 0 else 0\nfalsos_negativos_rate = fn / (fn + tp) if (fn + tp) &gt; 0 else 0\n\nprint(\"&lt;br&gt;&lt;b&gt;M\u00e9tricas:&lt;/b&gt;&lt;br&gt;\")\nprint(f\"Acur\u00e1cia: {acuracia:.4f}&lt;br&gt;\")\nprint(f\"Sensibilidade (Recall): {sensibilidade:.4f}&lt;br&gt;\")\nprint(f\"Especificidade: {especificidade:.4f}&lt;br&gt;\")\nprint(f\"Precis\u00e3o: {precisao:.4f}&lt;br&gt;\")\nprint(f\"Valor Preditivo Negativo: {valor_preditivo_negativo:.4f}&lt;br&gt;\")\nprint(f\"Taxa de Falsos Positivos: {falsos_positivos_rate:.4f}&lt;br&gt;\")\nprint(f\"Taxa de Falsos Negativos: {falsos_negativos_rate:.4f}\")\n</code></pre> <p>O modelo apresenta desempenho s\u00f3lido com acur\u00e1cia de 85.41%, indicando boa capacidade geral de classifica\u00e7\u00e3o.</p>"},{"location":"metrics/main/#pontos-fortes","title":"Pontos Fortes","text":"<ul> <li> <p>Alta Sensibilidade (90.55%): Excelente em identificar casos positivos (apenas 9.45% de falsos negativos)</p> </li> <li> <p>Boa Precis\u00e3o (87.99%): Quando prediz positivo, est\u00e1 correto 88% das vezes</p> </li> <li> <p>Alto VP Negativo (79.70%): Boa confiabilidade nas predi\u00e7\u00f5es negativas</p> </li> </ul>"},{"location":"metrics/main/#areas-de-atencao","title":"\u00c1reas de Aten\u00e7\u00e3o","text":"<ul> <li> <p>Especificidade Moderada (75.02%): O modelo tem dificuldade em identificar corretamente os casos negativos</p> </li> <li> <p>Taxa de Falsos Positivos (24.98%): Relativamente alta - 1 em cada 4 predi\u00e7\u00f5es positivas est\u00e1 errada</p> </li> </ul>"},{"location":"metrics/main/#curva-roc-e-auc","title":"Curva ROC e AUC","text":"Curva ROCC\u00f3digo AUC (Area Under Curve): 0.8891 <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\nfrom sklearn.metrics import roc_curve, auc\n\nencoder = OneHotEncoder()\nscaler = StandardScaler()\nl_encoder = LabelEncoder()\n\ndf = pd.read_csv(\"docs/knn/booking.csv\")\n\ndf = df.drop(columns=[\"Booking_ID\", \"date of reservation\"])\n\nnumeric_cols = [\"number of adults\", \"number of children\", \"number of weekend nights\", \n                \"number of week nights\", \"lead time\", \"P-C\", \"P-not-C\", \n                \"average price\", \"special requests\"]\n\ncategorical_cols = [\"type of meal\", \"room type\", \"market segment type\"]\n\nX = df.drop(\"booking status\", axis=1)\ny = l_encoder.fit_transform(df[\"booking status\"])\n\nfor col in numeric_cols:\n    X[col] = scaler.fit_transform(X[[col]])\n\nX_encoded = encoder.fit_transform(X[categorical_cols])\n\nencoded_df = pd.DataFrame(X_encoded.toarray(), columns=encoder.get_feature_names_out(categorical_cols), index=X.index)\n\nX = pd.concat([X.drop(columns=categorical_cols), encoded_df], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, y_train)\npredictions = knn.predict(X_test)\n\ny_prob = knn.predict_proba(X_test)[:, 1]\n\nfpr, tpr, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(fpr, tpr)\n\nplt.figure(figsize=(10, 8))\nplt.plot(fpr, tpr, color=\"darkorange\", lw=2, label=f\"Curva ROC (AUC = {roc_auc:.4f})\")\nplt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\", label=\"Classificador Aleat\u00f3rio\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel(\"Taxa de Falsos Positivos (1 - Especificidade)\")\nplt.ylabel(\"Taxa de Verdadeiros Positivos (Sensibilidade)\")\nplt.title(\"Curva ROC - KNN\")\nplt.legend(loc=\"lower right\")\nplt.grid(True, alpha=0.3)\n\nplt.legend()\nplt.savefig(\"docs/images/roc1.svg\", format=\"svg\", transparent=True)\nplt.close()\n\nprint(f\"\\nAUC (Area Under Curve): {roc_auc:.4f}\")\n</code></pre> <p>o AUC obtido, de 88,91%, \u00e9 classificado como excelente na escala de discrimina\u00e7\u00e3o, estando bem acima da linha do acaso e muito pr\u00f3ximo do canto superior esquerdo do gr\u00e1fico, o que indica bom trade-off entre sensibilidade e especificidade.</p>"},{"location":"metrics/main/#resultados-do-modelo-2","title":"Resultados do Modelo 2","text":"<p>O Modelo 2 pode ser visualizado da seguinte forma:</p> <pre><code>flowchart TD\n    A[Explora\u00e7\u00e3o de Dados] --&gt; B{Divis\u00e3o dos Dados}\n    B --&gt;|Teste&lt;br&gt;20% dos dados| PTest[Pr\u00e9-processamento]\n    B --&gt;|Treino&lt;br&gt;80% dos dados| PTrain[Pr\u00e9-processamento]\n    PTrain --&gt; Train[Treinamento]\n    Train --&gt; G\n    PTest --&gt; G[Avalia\u00e7\u00e3o do modelo]</code></pre> <p>Seu resultado pode ser visualizado abaixo:</p> Acur\u00e1cia: 0.8521"},{"location":"metrics/main/#metricas-para-o-modelo-2","title":"M\u00e9tricas para o Modelo 2","text":"<p>Agora, vamos descrever as m\u00e9tricas para o modelo, e explic\u00e1-las.</p>"},{"location":"metrics/main/#matriz-de-confusao_1","title":"Matriz de Confus\u00e3o","text":"<p>A matriz de confus\u00e3o consegue nos oferecer diversos valores que ser\u00e3o utilizados para calcular as m\u00e9tricas de qualidade.</p>"},{"location":"metrics/main/#metricas-de-qualidade_1","title":"M\u00e9tricas de Qualidade","text":"M\u00e9tricas - Modelo 2C\u00f3digo <p>Matriz de confus\u00e3o detalhada: Verdadeiros Negativos (TN): 1788 Falsos Positivos (FP): 614 Falsos Negativos (FN): 459 Verdadeiros Positivos (TP): 4396 M\u00e9tricas: Acur\u00e1cia: 0.8521 Sensibilidade (Recall): 0.9055 Especificidade: 0.7444 Precis\u00e3o: 0.8774 Valor Preditivo Negativo: 0.7957 Taxa de Falsos Positivos: 0.2556 Taxa de Falsos Negativos: 0.0945</p> <pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\nfrom sklearn.metrics import confusion_matrix\n\nencoder = OneHotEncoder()\nscaler = StandardScaler()\nl_encoder = LabelEncoder()\n\ndf = pd.read_csv(\"docs/knn/booking.csv\")\n\ndf = df.drop(columns=[\"Booking_ID\", \"date of reservation\"])\n\nnumeric_cols = [\"number of adults\", \"number of children\", \"number of weekend nights\", \n                \"number of week nights\", \"lead time\", \"P-C\", \"P-not-C\", \n                \"average price\", \"special requests\"]\n\ncategorical_cols = [\"type of meal\", \"room type\", \"market segment type\"]\n\nX = df.drop(\"booking status\", axis=1)\ny = l_encoder.fit_transform(df[\"booking status\"])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nscaler.fit(X_train[numeric_cols])\nencoder.fit(X_train[categorical_cols])\n\nX_train_scaled = scaler.transform(X_train[numeric_cols])\nX_test_scaled = scaler.transform(X_test[numeric_cols])\n\nX_train_encoded = encoder.transform(X_train[categorical_cols]).toarray()\nX_test_encoded = encoder.transform(X_test[categorical_cols]).toarray()\n\nX_train_final = np.concatenate([X_train_scaled, X_train_encoded], axis=1)\nX_test_final = np.concatenate([X_test_scaled, X_test_encoded], axis=1)\n\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train_final, y_train)\npredictions = knn.predict(X_test_final)\n\ncm = confusion_matrix(y_test, predictions)\ntn, fp, fn, tp = cm.ravel()\n\nprint(\"&lt;b&gt;Matriz de confus\u00e3o detalhada:&lt;/b&gt;&lt;br&gt;\")\nprint(f\"Verdadeiros Negativos (TN): {tn}&lt;br&gt;\")\nprint(f\"Falsos Positivos (FP): {fp}&lt;br&gt;\")\nprint(f\"Falsos Negativos (FN): {fn}&lt;br&gt;\")\nprint(f\"Verdadeiros Positivos (TP): {tp}&lt;br&gt;\")\n\nacuracia = (tn + tp) / (tn + fp + fn + tp)\nsensibilidade = tp / (tp + fn) if (tp + fn) &gt; 0 else 0\nespecificidade = tn / (tn + fp) if (tn + fp) &gt; 0 else 0\nprecisao = tp / (tp + fp) if (tp + fp) &gt; 0 else 0\nvalor_preditivo_negativo = tn / (tn + fn) if (tn + fn) &gt; 0 else 0\nfalsos_positivos_rate = fp / (fp + tn) if (fp + tn) &gt; 0 else 0\nfalsos_negativos_rate = fn / (fn + tp) if (fn + tp) &gt; 0 else 0\n\nprint(\"&lt;br&gt;&lt;b&gt;M\u00e9tricas:&lt;/b&gt;&lt;br&gt;\")\nprint(f\"Acur\u00e1cia: {acuracia:.4f}&lt;br&gt;\")\nprint(f\"Sensibilidade (Recall): {sensibilidade:.4f}&lt;br&gt;\")\nprint(f\"Especificidade: {especificidade:.4f}&lt;br&gt;\")\nprint(f\"Precis\u00e3o: {precisao:.4f}&lt;br&gt;\")\nprint(f\"Valor Preditivo Negativo: {valor_preditivo_negativo:.4f}&lt;br&gt;\")\nprint(f\"Taxa de Falsos Positivos: {falsos_positivos_rate:.4f}&lt;br&gt;\")\nprint(f\"Taxa de Falsos Negativos: {falsos_negativos_rate:.4f}\")\n</code></pre> <p>A acur\u00e1cia, pontos fortes e pontos de aten\u00e7\u00e3o s\u00e3o todos extremamente semelhantes ao modelo anterior, com varia\u00e7\u00f5es nas m\u00e9tricas abaixo de 1% em todos os casos.</p>"},{"location":"metrics/main/#curva-roc-e-auc_1","title":"Curva ROC e AUC","text":"Curva ROCC\u00f3digo AUC (Area Under Curve): 0.8876 <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\nfrom sklearn.metrics import roc_curve, auc\n\nencoder = OneHotEncoder()\nscaler = StandardScaler()\nl_encoder = LabelEncoder()\n\ndf = pd.read_csv(\"docs/knn/booking.csv\")\n\ndf = df.drop(columns=[\"Booking_ID\", \"date of reservation\"])\n\nnumeric_cols = [\"number of adults\", \"number of children\", \"number of weekend nights\", \n                \"number of week nights\", \"lead time\", \"P-C\", \"P-not-C\", \n                \"average price\", \"special requests\"]\n\ncategorical_cols = [\"type of meal\", \"room type\", \"market segment type\"]\n\nX = df.drop(\"booking status\", axis=1)\ny = l_encoder.fit_transform(df[\"booking status\"])\n\nfor col in numeric_cols:\n    X[col] = scaler.fit_transform(X[[col]])\n\nX_encoded = encoder.fit_transform(X[categorical_cols])\n\nencoded_df = pd.DataFrame(X_encoded.toarray(), columns=encoder.get_feature_names_out(categorical_cols), index=X.index)\n\nX = pd.concat([X.drop(columns=categorical_cols), encoded_df], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, y_train)\npredictions = knn.predict(X_test)\n\ny_prob = knn.predict_proba(X_test)[:, 1]\n\nfpr, tpr, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(fpr, tpr)\n\nplt.figure(figsize=(10, 8))\nplt.plot(fpr, tpr, color=\"darkorange\", lw=2, label=f\"Curva ROC (AUC = {roc_auc:.4f})\")\nplt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\", label=\"Classificador Aleat\u00f3rio\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel(\"Taxa de Falsos Positivos (1 - Especificidade)\")\nplt.ylabel(\"Taxa de Verdadeiros Positivos (Sensibilidade)\")\nplt.title(\"Curva ROC - KNN\")\nplt.legend(loc=\"lower right\")\nplt.grid(True, alpha=0.3)\n\nplt.legend()\nplt.savefig(\"docs/images/roc1.svg\", format=\"svg\", transparent=True)\nplt.close()\n\nprint(f\"\\nAUC (Area Under Curve): {roc_auc:.4f}\")\n</code></pre> <p>o AUC obtido, de 88,76%, \u00e9 classificado como excelente na escala de discrimina\u00e7\u00e3o, da mesma forma que o AUC do modelo 1. Novamente, uma diferen\u00e7a min\u00fascula entre as m\u00e9tricas dos dois modelos, com varia\u00e7\u00e3o de apenas 0,15%.</p>"},{"location":"metrics/main/#modelo-k-means","title":"Modelo K-Means","text":"<p>Agora, faremos a an\u00e1lise do modelo K-Means. O dataset utilizado possui dados sint\u00e9ticamente gerados sobre peixes. O objetivo do projeto foi clusterizar esses peixes atrav\u00e9s, puramente, de suas caracter\u00edsticas f\u00edsicas.</p>"},{"location":"metrics/main/#elbow-method","title":"Elbow Method","text":"<p>Antes do treinamento do modelo, foi necess\u00e1rio descobrir o n\u00famero de clusters que seriam utilizado. Para isso, foi aplicado o Elbow Method, que definiu o n\u00famero de cluster em \\(k = 3\\), destacando que a separa\u00e7\u00e3o puramente f\u00edsica se diverge da separa\u00e7\u00e3o de esp\u00e9cies biol\u00f3gicas, j\u00e1 que existem 9 esp\u00e9cies de peixes distintas na base de dados.</p> ElbowC\u00f3digo <p></p> <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\ndf = pd.read_csv(\"docs/k-means/fish_data.csv\")\n\nX = df.drop(columns=[\"species\"])\n\nX = scaler.fit_transform(X)\n\n# Elbow Method\n\nwcss = []\nk_range = range(1, 11)\n\nfor k in k_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    kmeans.fit(X)\n    wcss.append(kmeans.inertia_)\n\nplt.figure(figsize=(10, 6))\nplt.plot(k_range, wcss, \"bo-\", markersize=8, linewidth=2)\nplt.xlabel(\"N\u00famero de Clusters (K)\")\nplt.ylabel(\"WCSS (Within-Cluster Sum of Square)\")\nplt.title(\"Elbow Method - Determinando o K ideal\")\nplt.grid(True, alpha=0.3)\nplt.xticks(k_range)\n\nplt.axvline(x=3, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"Poss\u00edvel cotovelo K=3\")\n\nplt.legend()\nplt.savefig(\"docs/images/elbow.svg\", format=\"svg\", transparent=True)\nplt.close()\n</code></pre>"},{"location":"metrics/main/#resultados-do-modelo","title":"Resultados do modelo","text":"Silhouette Score: 0.6284"},{"location":"metrics/main/#metricas-para-o-modelo","title":"M\u00e9tricas para o modelo","text":""},{"location":"metrics/main/#silhouette-score","title":"Silhouette Score","text":"<p>O modelo alcan\u00e7ou um Silhouette Score de 0.6284, indicando uma estrutura de clusters bem definida e distinta. Na escala de -1 a +1, este valor se enquadra na categoria Boa a Forte, sugerindo que os clusters possuem alta coes\u00e3o interna e boa separa\u00e7\u00e3o entre si, com sobreposi\u00e7\u00e3o m\u00ednima entre os grupos.</p>"},{"location":"metrics/main/#variancia-explicada","title":"Vari\u00e2ncia Explicada","text":"<p>O PCA aplicado para visualiza\u00e7\u00e3o explica 98.80% da vari\u00e2ncia total dos dados, com o primeiro componente (PC1) capturando 58.68% e o segundo componente (PC2) 40.12%. Isso indica que a visualiza\u00e7\u00e3o 2D representa fielmente a estrutura multidimensional original dos dados de medidas dos peixes.</p>"},{"location":"metrics/main/#tamanho-dos-clusters","title":"Tamanho dos Clusters","text":"<p>A distribui\u00e7\u00e3o de peixes entre os clusters pode ser visualizada com o c\u00f3digo abaixo:</p> Cluster sizeC\u00f3digo <p>Tamanho dos clusters: [ 455 1319 2306]</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\ndf = pd.read_csv(\"docs/k-means/fish_data.csv\")\n\nX = df.drop(columns=[\"species\"])\n\nfor col in X:\n    X[col] = scaler.fit_transform(X[[col]])\n\nkmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\ncluster_labels = kmeans.fit_predict(X)\n\nprint(f\"Tamanho dos clusters: {np.bincount(cluster_labels)}\")\n</code></pre> <p>Dessa distribui\u00e7\u00e3o, podemos tirar alguns pontos positivos:</p> <ul> <li> <p>Nenhum cluster min\u00fasculo (todos acima de 10% dos dados)</p> </li> <li> <p>Distribui\u00e7\u00e3o progressiva - clusters bem diferenciados em tamanho</p> </li> <li> <p>Sem clusters extremamente desbalanceados</p> </li> </ul> <p>Al\u00e9m disso, foi identificado um padr\u00e3o:</p> <ul> <li> <p>Cluster 2 (56.3%) \u2192 Maior grupo</p> </li> <li> <p>Cluster 1 (32.2%) \u2192 Grupo intermedi\u00e1rio</p> </li> <li> <p>Cluster 0 (11.1%) \u2192 Menor grupo</p> </li> </ul>"},{"location":"metrics/main/#conclusao","title":"Conclus\u00e3o","text":"<p>A an\u00e1lise comparativa dos modelos KNN e K-Means revelou insights valiosos sobre suas respectivas aplica\u00e7\u00f5es e desempenhos. O KNN demonstrou excelente capacidade preditiva para classifica\u00e7\u00e3o de cancelamentos de reservas hoteleiras, com ambos os modelos (com e sem data leakage) apresentando m\u00e9tricas s\u00f3lidas - acur\u00e1cia acima de 85% e AUC superior a 88%, indicando robustez na discrimina\u00e7\u00e3o entre classes.</p> <p>J\u00e1 o K-Means mostrou-se eficaz na identifica\u00e7\u00e3o de padr\u00f5es naturais em dados de medidas f\u00edsicas de peixes, formando 3 clusters bem definidos com Silhouette Score de 0.6284. A descoberta mais significativa foi que o agrupamento natural por caracter\u00edsticas f\u00edsicas difere da classifica\u00e7\u00e3o biol\u00f3gica por esp\u00e9cies, revelando que diferentes esp\u00e9cies compartilham perfis dimensionais similares.</p> <p>Ambos os algoritmos comprovaram seu valor: o KNN como ferramenta confi\u00e1vel para problemas de classifica\u00e7\u00e3o supervisionada, e o K-Means como m\u00e9todo eficaz para descoberta de padr\u00f5es intr\u00ednsecos em dados n\u00e3o rotulados, cada um atendendo a objetivos distintos com performances satisfat\u00f3rias em seus respectivos contextos.</p>"},{"location":"random-forest/main/","title":"Projeto 5 - Random Forest","text":""},{"location":"random-forest/main/#modelo-de-machine-learning-knn","title":"Modelo de Machine Learning - KNN","text":"<p>Para esse projeto, foi utilizado um dataset obtido no Kaggle. Os dados usados podem ser baixados aqui.</p>"},{"location":"random-forest/main/#objetivo","title":"Objetivo","text":"<p>O dataset utilizado possui informa\u00e7\u00f5es sobre pacientes sob efeito de drogas farmac\u00eauticas. O objetivo da predi\u00e7\u00e3o \u00e9 prever o tipo de droga baseado nas features do modelo.</p>"},{"location":"random-forest/main/#workflow","title":"Workflow","text":"<p>Os pontos \"etapas\" s\u00e3o o passo-a-passo da realiza\u00e7\u00e3o do projeto.</p>"},{"location":"random-forest/main/#etapa-1-exploracao-de-dados","title":"Etapa 1 - Explora\u00e7\u00e3o de Dados","text":"<p>Primeiramente, deve ser feita a explora\u00e7\u00e3o dos dados da base, com o objetivo de compreender a forma como s\u00e3o estruturados os dados, sua natureza e poss\u00edvel signific\u00e2ncia para o modelo de predi\u00e7\u00e3o.</p> <p>O dataset \u00e9 composto por 200 linhas e 6 colunas, com cada linha representando um paciente distinto. Essa verifica\u00e7\u00e3o p\u00f4de ser feita com as linhas de c\u00f3digo abaixo;</p> Sa\u00eddaC\u00f3digo <p>(200, 6)</p> <pre><code>import pandas as pd\n\ndf = pd.read_csv(\"docs/random-forest/drug200.csv\")\n\nprint(df.shape)\n</code></pre>"},{"location":"random-forest/main/#colunas-do-dataset","title":"Colunas do dataset","text":"Coluna Tipo Descri\u00e7\u00e3o Age Inteiro Idade do paciente Sex String G\u00eanero do paciente BP String N\u00edveis de press\u00e3o sangu\u00ednea Cholesterol String N\u00edveis de colesterol Na_to_K Float Raz\u00e3o do s\u00f3dio para o pot\u00e1ssio no sangue Drug String Tipo de droga"},{"location":"random-forest/main/#visualizacoes-das-variaveis","title":"Visualiza\u00e7\u00f5es das vari\u00e1veis","text":"<p>Em seguida, \u00e9 essencial realizar gr\u00e1ficos para visualizar como cada uma das vari\u00e1veis se comportam, com o objetivo de entender melhor a base da dados.</p> <p>Est\u00e1 se\u00e7\u00e3o ser\u00e1 divida para cada tipo de vari\u00e1vel, entre vari\u00e1veis quantitativas discretas, quantitativas cont\u00ednuas, categ\u00f3ricas e, por fim, a vari\u00e1vel alvo.</p>"},{"location":"random-forest/main/#variaveis-categoricas","title":"Vari\u00e1veis Categ\u00f3ricas","text":"SexBP Gr\u00e1ficoC\u00f3digo 2025-10-29T22:00:24.048908 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/random-forest/drug200.csv\")\n\nplt.figure(figsize=(10, 6))\ndf[\"Sex\"].value_counts().sort_index().plot(kind=\"bar\", color=\"lightyellow\", edgecolor=\"black\")\nplt.title(\"G\u00eanero dos Pacientes - Barras\")\nplt.xlabel(\"G\u00eanero\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.xticks(rotation=0)\nplt.grid(axis=\"y\", alpha=0.3)\n\nax = plt.gca()\nfor p in ax.patches:\n    ax.annotate(f\"{int(p.get_height())}\", \n                (p.get_x() + p.get_width()/2., p.get_height()),\n                ha=\"center\", va=\"bottom\")\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> Gr\u00e1ficoC\u00f3digo 2025-10-29T22:00:24.182672 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/random-forest/drug200.csv\")\n\nplt.figure(figsize=(10, 6))\ndf[\"BP\"].value_counts().sort_index().plot(kind=\"bar\", color=\"skyblue\", edgecolor=\"black\")\nplt.title(\"Press\u00e3o Sangu\u00ednea dos Pacientes - Barras\")\nplt.xlabel(\"Press\u00e3o Sangu\u00ednea\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.xticks(rotation=0)\nplt.grid(axis=\"y\", alpha=0.3)\n\nax = plt.gca()\nfor p in ax.patches:\n    ax.annotate(f\"{int(p.get_height())}\", \n                (p.get_x() + p.get_width()/2., p.get_height()),\n                ha=\"center\", va=\"bottom\")\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre>"},{"location":"random-forest/main/#variavel-quantitativa-discreta-age","title":"Vari\u00e1vel Quantitativa Discreta <code>Age</code>","text":"Gr\u00e1ficoC\u00f3digo 2025-10-29T22:00:24.339377 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/random-forest/drug200.csv\")\n\nplt.figure(figsize=(10, 6))\nplt.hist(df[\"Age\"], bins=30, edgecolor=\"black\", alpha=0.7, color=\"red\")\nplt.title(\"Idade dos Pacientes - Histograma\")\nplt.xlabel(\"Idade\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.grid(axis=\"y\", alpha=0.3)\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre>"},{"location":"random-forest/main/#variavel-quantitativa-continua-na_to_k","title":"Vari\u00e1vel Quantitativa Cont\u00ednua <code>Na_to_K</code>","text":"Gr\u00e1ficoC\u00f3digo 2025-10-29T22:00:24.508462 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/random-forest/drug200.csv\")\n\nplt.figure(figsize=(10, 6))\nplt.hist(df[\"Na_to_K\"], bins=30, edgecolor=\"black\", alpha=0.7, color=\"lightblue\")\nplt.title(\"Distribui\u00e7\u00e3o da Raz\u00e3o de S\u00f3dio para Pot\u00e1ssio no Sangue dos Pacientes - Histograma\")\nplt.xlabel(\"Raz\u00e3o de S\u00f3dio para Pot\u00e1ssio no Sangue\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.grid(axis=\"y\", alpha=0.3)\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre>"},{"location":"random-forest/main/#variavel-alvo-drug","title":"Vari\u00e1vel Alvo <code>Drug</code>","text":"Gr\u00e1ficoC\u00f3digo 2025-10-29T22:00:24.672423 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/random-forest/drug200.csv\")\n\nplt.figure(figsize=(10, 6))\ndf[\"Drug\"].value_counts().sort_index().plot(kind=\"bar\", color=\"lightgreen\", edgecolor=\"black\")\nplt.title(\"Quantidade dos Tipos De Droga nos Pacientes - Barras\")\nplt.xlabel(\"Tipos de Droga\")\nplt.ylabel(\"Quantidade\")\nplt.xticks(rotation=0)\nplt.grid(axis=\"y\", alpha=0.3)\n\nax = plt.gca()\nfor p in ax.patches:\n    ax.annotate(f\"{int(p.get_height())}\", \n                (p.get_x() + p.get_width()/2., p.get_height()),\n                ha=\"center\", va=\"bottom\")\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> <p>Atrav\u00e9s das an\u00e1lises, foi poss\u00edvel alcan\u00e7ar uma compreens\u00e3o mais aprofundada do funcionamento de cada uma das vari\u00e1veis no dataset, al\u00e9m de haver insights valiosos nesses gr\u00e1ficos.</p>"},{"location":"random-forest/main/#etapa-2-pre-processamento","title":"Etapa 2 - Pr\u00e9-processamento","text":"<p>Nessa etapa, vamos tratar a base para uso no treinamento do modelo.</p>"},{"location":"random-forest/main/#1-passo-identificacao-e-tratamento-de-valores-nulos","title":"1\u00b0 Passo: Identifica\u00e7\u00e3o e tratamento de valores nulos","text":"<p>O primeiro passo para o pr\u00e9-processamento \u00e9 identificar e tratar valores nulos na base.</p> <pre><code>print(df.isna().sum())\n</code></pre> <p>Executando a linha de c\u00f3digo acima para o dataframe contendo os dados da base, foi poss\u00edvel identificar que n\u00e3o h\u00e1 valores nulos na base.</p>"},{"location":"random-forest/main/#2-passo-codificacao-de-variaveis-categoricas","title":"2\u00b0 Passo: Codifica\u00e7\u00e3o de vari\u00e1veis categ\u00f3ricas","text":"<p>O segundo passo se consiste na codifica\u00e7\u00e3o das vari\u00e1veis categ\u00f3ricas. Essas s\u00e3o: <code>Sex</code>, <code>BP</code> e <code>Cholesterol</code>. Utilizaremos a t\u00e9cnica de One-Hot Encoding para codificar essas vari\u00e1veis, utilizando o OneHotEncoder() do <code>scikit-learn</code>.</p> <pre><code>from sklearn.preprocessing import OneHotEncoder\n\nencoder = OneHotEncoder()\ncategorical_cols = [\"Sex\", \"BP\", \"Cholesterol\"]\n\nX = df.drop(\"Drug\", axis=1)\n\nX_encoded = encoder.fit_transform(X[categorical_cols])\nencoded_df = pd.DataFrame(X_encoded.toarray(), columns=encoder.get_feature_names_out(categorical_cols), index=X.index)\n\nX = pd.concat([X.drop(columns=categorical_cols), encoded_df], axis=1)\n</code></pre>"},{"location":"random-forest/main/#3-passo-padronizacao-das-features-numericas","title":"3\u00b0 Passo: Padroniza\u00e7\u00e3o das features num\u00e9ricas","text":"<p>Em seguida, \u00e9 necess\u00e1ria a padroniza\u00e7\u00e3o das features num\u00e9ricas na base. Utilizaremos o StandardScaler() do <code>scikit-learn</code> para padronizar as vari\u00e1veis <code>Na_to_K</code> e <code>Age</code>.</p> <pre><code>from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nnumeric_cols = [\"Age\", \"Na_to_K\"]\n\nX = df.drop(\"Drug\", axis=1)\n\nX_scaled = scaler.fit_transform(X[numeric_cols])\nscaled_df = pd.DataFrame(X_scaled, columns=numeric_cols, index=X.index)\n\nX = pd.concat([X.drop(columns=numeric_cols), scaled_df], axis=1)\n</code></pre>"},{"location":"random-forest/main/#4-passo-codificacao-da-variavel-alvo","title":"4\u00b0 Passo: Codifica\u00e7\u00e3o da vari\u00e1vel alvo","text":"<p>Por fim, vamos codificar a vari\u00e1vel alvo <code>Drug</code> utilizando a t\u00e9cnica de label encoding. Para codificar, utilizaremos o LabelEncoder() do <code>scikit-learn</code>.</p> <pre><code>from sklearn.preprocessing import LabelEncoder\n\nencoder = LabelEncoder()\ny = encoder.fit_transform(df[\"Drug\"])\n</code></pre>"},{"location":"random-forest/main/#etapa-3-divisao-dos-dados","title":"Etapa 3 - Divis\u00e3o dos dados","text":"<p>Em seguida, vamos realizar a divis\u00e3o dos dados em conjuntos de treino e teste.</p> <ul> <li> <p>Conjunto de Treino: Utilizado para ensinar o modelo a reconhecer padr\u00f5es</p> </li> <li> <p>Conjunto de Teste: Utilizado para avaliar o desempenho do modelo com dados ainda n\u00e3o vistos</p> </li> </ul> <p>Para realizar a divis\u00e3o, foi utilizada a fun\u00e7\u00e3o train_test_split() do <code>scikit-learn</code>. Os par\u00e2metros utilizados s\u00e3o:</p> <ul> <li> <p>test_size=0.2: Define que 20% dos dados ser\u00e3o utilizados para teste, enquanto o restante ser\u00e1 usado para treino.</p> </li> <li> <p>random_state=42: Par\u00e2metro que controla o gerador de n\u00famero aleat\u00f3rios utilizado para sortear os dados antes de separ\u00e1-los. Garante reprodutibilidade.</p> </li> <li> <p>stratify=y: Esse atributo definido como y \u00e9 essencial devido \u00e0 natureza da coluna <code>Drug</code>. Com essa defini\u00e7\u00e3o, ser\u00e1 mantida a mesma propor\u00e7\u00e3o das categorias em ambos os conjuntos, reduzindo o vi\u00e9s.</p> </li> </ul> Sa\u00eddaC\u00f3digo <p>Treino: 160 amostras</p> <p>Teste: 40 amostras</p> <p>Propor\u00e7\u00e3o: 80.0% treino, 20.0% teste</p> <p>Distribui\u00e7\u00e3o das classes - </p> <p>Treino:</p> count 0 73 4 43 1 18 2 13 3 13 <p>Teste:</p> count 0 18 4 11 1 5 3 3 2 3 <pre><code>import pandas as pd\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\ndf = pd.read_csv(\"docs/random-forest/drug200.csv\")\n\nencoder = OneHotEncoder()\nl_encoder = LabelEncoder()\nscaler = StandardScaler()\n\nnumeric_cols = [\"Age\", \"Na_to_K\"]\ncategorical_cols = [\"Sex\", \"BP\", \"Cholesterol\"]\n\nX = df.drop(\"Drug\", axis=1)\n\nX_encoded = encoder.fit_transform(X[categorical_cols])\nencoded_df = pd.DataFrame(X_encoded.toarray(), columns=encoder.get_feature_names_out(categorical_cols), index=X.index)\n\nX_scaled = scaler.fit_transform(X[numeric_cols])\nscaled_df = pd.DataFrame(X_scaled, columns=numeric_cols, index=X.index)\n\nX = pd.concat([scaled_df, encoded_df], axis=1)\n\ny = l_encoder.fit_transform(df[\"Drug\"])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\nprint(f\"Treino: {X_train.shape[0]} amostras\\n\")\nprint(f\"Teste: {X_test.shape[0]} amostras\\n\")\nprint(f\"Propor\u00e7\u00e3o: {X_train.shape[0]/X.shape[0]*100:.1f}% treino, {X_test.shape[0]/X.shape[0]*100:.1f}% teste\\n\")\n\nprint(\"Distribui\u00e7\u00e3o das classes - \\n\")\nprint(\"Treino:\\n\")\nprint(pd.Series(y_train).value_counts().to_markdown(), \"\\n\")\nprint(\"Teste:\\n\")\nprint(pd.Series(y_test).value_counts().to_markdown(), \"\\n\")\n</code></pre> <p>Esta divis\u00e3o adequada \u00e9 de extrema import\u00e2ncia, pois ajuda a evitar overfitting.</p>"},{"location":"random-forest/main/#etapa-4-treinamento-do-modelo","title":"Etapa 4 - Treinamento do Modelo","text":"<p>Agora, ser\u00e1 realizado o treinamento do modelo. O objetivo dessa etapa \u00e9 ensinar o algoritmo a reconhecer padr\u00f5es nos dados que s\u00e3o fornecidos, e prever o tipo de droga presente no sangue dos pacientes atrav\u00e9s das features do modelo.</p> Sa\u00eddaC\u00f3digo <p>Accuracy: 0.975 Import\u00e2ncia das Features:  Feature Import\u00e2ncia 1 Na_to_K 0.516979 4 BP_HIGH 0.133267 0 Age 0.112744 6 BP_NORMAL 0.069356 5 BP_LOW 0.057098 7 Cholesterol_HIGH 0.046899 8 Cholesterol_NORMAL 0.041138 3 Sex_M 0.013216 2 Sex_F 0.009304 </p> <pre><code>import pandas as pd\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\ndf = pd.read_csv(\"docs/random-forest/drug200.csv\")\n\nencoder = OneHotEncoder()\nl_encoder = LabelEncoder()\nscaler = StandardScaler()\n\nnumeric_cols = [\"Age\", \"Na_to_K\"]\ncategorical_cols = [\"Sex\", \"BP\", \"Cholesterol\"]\n\nX = df.drop(\"Drug\", axis=1)\n\nX_encoded = encoder.fit_transform(X[categorical_cols])\nencoded_df = pd.DataFrame(X_encoded.toarray(), columns=encoder.get_feature_names_out(categorical_cols), index=X.index)\n\nX_scaled = scaler.fit_transform(X[numeric_cols])\nscaled_df = pd.DataFrame(X_scaled, columns=numeric_cols, index=X.index)\n\nX = pd.concat([scaled_df, encoded_df], axis=1)\n\ny = l_encoder.fit_transform(df[\"Drug\"])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\nrf = RandomForestClassifier(n_estimators=100,\n                            max_depth=5,\n                            max_features='sqrt', \n                            random_state=42)\n\nrf.fit(X_train, y_train)\n\npredictions = rf.predict(X_test)\nprint(f\"Accuracy: {accuracy_score(y_test, predictions)}\")\n\nfeature_importance = pd.DataFrame({\n    \"Feature\": rf.feature_names_in_,\n    \"Import\u00e2ncia\": rf.feature_importances_\n})\nprint(\"&lt;br&gt;Import\u00e2ncia das Features:\")\nprint(feature_importance.sort_values(by=\"Import\u00e2ncia\", ascending=False).to_html() + \"&lt;br&gt;\")\n</code></pre>"},{"location":"random-forest/main/#etapa-5-avaliacao-do-modelo","title":"Etapa 5 - Avalia\u00e7\u00e3o do modelo","text":"<p>Nessa etapa, vamos avaliar os resultados obtidos pelo modelo Random Forest.</p>"},{"location":"random-forest/main/#acuracia-do-modelo","title":"Acur\u00e1cia do modelo","text":"<p>Como \u00e9 poss\u00edvel observar acima na etapa de treinamento do modelo, o modelo atingiu uma alta acur\u00e1cia, de 97,5%, indicando que, possivelmente, o modelo sofre de overfitting. Ou seja, existe a chance do modelo estar apenas memorizando os dados ao inv\u00e9s de entend\u00ea-los, devido a um poss\u00edvel ru\u00eddo dos dados de treino. Por isso, vamor realizar um teste de valida\u00e7\u00e3o cruzada para testar se o modelo sofre desse problema.</p>"},{"location":"random-forest/main/#teste-de-validacao-cruzada","title":"Teste de valida\u00e7\u00e3o cruzada","text":"Sa\u00eddaC\u00f3digo <p>Valida\u00e7\u00e3o Cruzada (5 folds):</p> <p>Scores: [1.  1.  1.  0.9 1. ]</p> <p>M\u00e9dia: 0.9800 (+/- 0.0800)</p> <p>Diferen\u00e7a para acur\u00e1cia original: -0.0050</p> <pre><code>import pandas as pd\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\ndf = pd.read_csv(\"docs/random-forest/drug200.csv\")\n\nencoder = OneHotEncoder()\nl_encoder = LabelEncoder()\nscaler = StandardScaler()\n\nnumeric_cols = [\"Age\", \"Na_to_K\"]\ncategorical_cols = [\"Sex\", \"BP\", \"Cholesterol\"]\n\nX = df.drop(\"Drug\", axis=1)\n\nX_encoded = encoder.fit_transform(X[categorical_cols])\nencoded_df = pd.DataFrame(X_encoded.toarray(), columns=encoder.get_feature_names_out(categorical_cols), index=X.index)\n\nX_scaled = scaler.fit_transform(X[numeric_cols])\nscaled_df = pd.DataFrame(X_scaled, columns=numeric_cols, index=X.index)\n\nX = pd.concat([scaled_df, encoded_df], axis=1)\n\ny = l_encoder.fit_transform(df[\"Drug\"])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\nrf = RandomForestClassifier(n_estimators=100,\n                            max_depth=5,\n                            max_features='sqrt', \n                            random_state=42)\n\nrf.fit(X_train, y_train)\n\npredictions = rf.predict(X_test)\n\ncv_scores = cross_val_score(rf, X, y, cv=5, scoring='accuracy')\nprint(f\"Valida\u00e7\u00e3o Cruzada (5 folds):\")\nprint(f\"\\nScores: {cv_scores}\")\nprint(f\"\\nM\u00e9dia: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\nprint(f\"\\nDiferen\u00e7a para acur\u00e1cia original: {accuracy_score(y_test, predictions) - cv_scores.mean():.4f}\")\n</code></pre> <p>O modelo demonstra excelente generaliza\u00e7\u00e3o, com acur\u00e1cia m\u00e9dia de 98% na valida\u00e7\u00e3o cruzada, ligeiramente superior aos 97,5% obtidos no teste inicial. A consist\u00eancia entre os folds \u00e9 alta, com quatro deles alcan\u00e7ando 100% de acur\u00e1cia e apenas um apresentando 90%.</p> <p>A pequena varia\u00e7\u00e3o (de aproximadamente 8%) entre os folds e a diferen\u00e7a insignificante de 0,5% entre a valida\u00e7\u00e3o cruzada e o teste original indicam que n\u00e3o h\u00e1 evid\u00eancias de overfitting. O modelo mostra robustez e capacidade de generaliza\u00e7\u00e3o adequada para os dados.</p>"},{"location":"random-forest/main/#importancia-das-features","title":"Import\u00e2ncia das features","text":"<p>Agora, vamos analisar a import\u00e2ncia das features do modelo.</p> <ul> <li> <p>As vari\u00e1veis mais importantes para a predi\u00e7\u00e3o foram <code>Na_to_K</code>, <code>BP_HIGH</code> e <code>Age</code>.</p> </li> <li> <p>Foi poss\u00edvel observar que as vari\u00e1veis dummy <code>Sex_M</code> e <code>Sex_F</code> possuem relev\u00e2ncia muito m\u00ednima para a predi\u00e7\u00e3o, de 1,32% e 0,93%, respectivamente.</p> </li> </ul>"},{"location":"random-forest/main/#matriz-de-confusao-e-metricas-de-qualidade","title":"Matriz de confus\u00e3o e M\u00e9tricas de Qualidade","text":"<p>Em seguida, vamos analisar a matriz de confus\u00e3o e obter as m\u00e9tricas de avalia\u00e7\u00e3o:</p> Matriz de Confus\u00e3oC\u00f3digo <p>Matriz de confus\u00e3o</p> <p></p> <p>M\u00e9tricas de qualidade</p> precision recall f1-score support 0 0.95 1 0.97 18 1 1 1 1 5 2 1 1 1 3 3 1 1 1 3 4 1 0.91 0.95 11 accuracy 0.98 0.98 0.98 0.98 macro avg 0.99 0.98 0.99 40 weighted avg 0.98 0.98 0.97 40 <pre><code>import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\n\ndf = pd.read_csv(\"docs/random-forest/drug200.csv\")\n\nencoder = OneHotEncoder()\nl_encoder = LabelEncoder()\nscaler = StandardScaler()\n\nnumeric_cols = [\"Age\", \"Na_to_K\"]\ncategorical_cols = [\"Sex\", \"BP\", \"Cholesterol\"]\n\nX = df.drop(\"Drug\", axis=1)\n\nX_encoded = encoder.fit_transform(X[categorical_cols])\nencoded_df = pd.DataFrame(X_encoded.toarray(), columns=encoder.get_feature_names_out(categorical_cols), index=X.index)\n\nX_scaled = scaler.fit_transform(X[numeric_cols])\nscaled_df = pd.DataFrame(X_scaled, columns=numeric_cols, index=X.index)\n\nX = pd.concat([scaled_df, encoded_df], axis=1)\n\ny = l_encoder.fit_transform(df[\"Drug\"])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\nrf = RandomForestClassifier(n_estimators=100,\n                            max_depth=5,\n                            max_features='sqrt', \n                            random_state=42)\n\nrf.fit(X_train, y_train)\n\npredictions = rf.predict(X_test)\n\ncm = confusion_matrix(y_test, predictions)\n\nplt.figure(figsize=(6,4))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=l_encoder.classes_, yticklabels=l_encoder.classes_)\nplt.xlabel(\"Predito\")\nplt.ylabel(\"Real\")\nplt.title(\"Matriz de Confus\u00e3o - Random Forest\")\n\n# plt.savefig(\"docs/images/cm-rf.svg\", format=\"svg\", transparent=True)\nplt.close()\n\nreport_dict = classification_report(y_test, predictions, output_dict=True)\nreport_df = pd.DataFrame(report_dict).transpose()\n\nprint(report_df.round(2).to_markdown())\n</code></pre> <p>Como \u00e9 poss\u00edvel observar acima, o modelo possui excelentes m\u00e9tricas de qualidade, j\u00e1 que das 40 amostras no conjunto de teste, apenas uma foi prevista incorretamente.</p>"},{"location":"random-forest/main/#etapa-6-relatorio-final","title":"Etapa 6 - Relat\u00f3rio Final","text":"<p>Para finalizar o projeto, vamos ao relat\u00f3rio final do modelo:</p>"},{"location":"random-forest/main/#recomendacoes-e-conclusoes","title":"Recomenda\u00e7\u00f5es e Conclus\u00f5es","text":"<p>O modelo Random Forest desenvolvido demonstra excelente desempenho e est\u00e1 pronto para uso em um ambiente controlado. Recomenda-se sua implementa\u00e7\u00e3o para classifica\u00e7\u00e3o de medicamentos com base nas caracter\u00edsticas dos pacientes, considerando sua alta acur\u00e1cia e robustez comprovadas.</p>"},{"location":"random-forest/main/#pontos-importantes-observados","title":"Pontos Importantes Observados","text":"<ul> <li> <p>Alta performance: O modelo alcan\u00e7ou 97,5% de acur\u00e1cia no teste inicial e 98% na valida\u00e7\u00e3o cruzada</p> </li> <li> <p>Boa generaliza\u00e7\u00e3o: A valida\u00e7\u00e3o cruzada confirmou a aus\u00eancia de overfitting significativo</p> </li> <li> <p>Feature mais relevante: A vari\u00e1vel <code>Na_to_K</code> mostrou-se como a mais importante para predi\u00e7\u00e3o</p> </li> </ul>"},{"location":"random-forest/main/#possiveis-proximos-passos-e-melhorias","title":"Poss\u00edveis Pr\u00f3ximos Passos e Melhorias","text":"<ul> <li> <p>Coleta de mais dados: Expandir o dataset para aumentar a robustez do modelo</p> </li> <li> <p>Engenharia de features: Explorar combina\u00e7\u00f5es entre vari\u00e1veis existentes</p> </li> <li> <p>Valida\u00e7\u00e3o externa: Testar o modelo com dados de fontes diferentes</p> </li> </ul>"},{"location":"random-forest/main/#conclusao-final","title":"Conclus\u00e3o Final","text":"<p>O projeto atingiu plenamente seu objetivo de desenvolver um modelo preditivo eficaz para classifica\u00e7\u00e3o de medicamentos. O Random Forest mostrou-se como uma escolha adequada, entregando resultados consistentes e confi\u00e1veis. A abordagem metodol\u00f3gica adotada, desde a explora\u00e7\u00e3o inicial at\u00e9 a valida\u00e7\u00e3o rigorosa, garantiu a qualidade do modelo final, que se apresenta como uma solu\u00e7\u00e3o vi\u00e1vel para o problema proposto.</p>"}]}